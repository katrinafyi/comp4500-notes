<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    
    <title>Single Page View</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/light.min.css">
    <!--<link rel="stylesheet" href="https://unpkg.com/mvp.css">-->
    <!--<link rel="stylesheet" href="https://latex.now.sh/style.css">-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
    <style>
      .katex { font-size: 1.1em; }
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
      div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
      ul.task-list{list-style: none;}
      pre > code.sourceCode { white-space: pre; position: relative; }
      pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
      pre > code.sourceCode > span:empty { height: 1.2em; }
      code.sourceCode > span { color: inherit; text-decoration: inherit; }
      div.sourceCode { margin: 1em 0; }
      pre.sourceCode { margin: 0; }
      @media screen {
      div.sourceCode { overflow: auto; }
      }
      @media print {
      pre > code.sourceCode { white-space: pre-wrap; }
      pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
      }
      pre.numberSource code
        { counter-reset: source-line 0; }
      pre.numberSource code > span
        { position: relative; left: -4em; counter-increment: source-line; }
      pre.numberSource code > span > a:first-child::before
        { content: counter(source-line);
          position: relative; left: -1em; text-align: right; vertical-align: baseline;
          border: none; display: inline-block;
          -webkit-touch-callout: none; -webkit-user-select: none;
          -khtml-user-select: none; -moz-user-select: none;
          -ms-user-select: none; user-select: none;
          padding: 0 4px; width: 4em;
          color: #aaaaaa;
        }
      pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
      div.sourceCode {   }
      @media screen {
        pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
      }
      code span.al { color: #ff0000; font-weight: bold; } /* Alert */
      code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
      code span.at { color: #7d9029; } /* Attribute */
      code span.bn { color: #40a070; } /* BaseN */
      code span.bu { } /* BuiltIn */
      code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
      code span.ch { color: #4070a0; } /* Char */
      code span.cn { color: #880000; } /* Constant */
      code span.co { color: #60a0b0; font-style: italic; } /* Comment */
      code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
      code span.do { color: #ba2121; font-style: italic; } /* Documentation */
      code span.dt { color: #902000; } /* DataType */
      code span.dv { color: #40a070; } /* DecVal */
      code span.er { color: #ff0000; font-weight: bold; } /* Error */
      code span.ex { } /* Extension */
      code span.fl { color: #40a070; } /* Float */
      code span.fu { color: #06287e; } /* Function */
      code span.im { } /* Import */
      code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
      code span.kw { color: #007020; font-weight: bold; } /* Keyword */
      code span.op { color: #666666; } /* Operator */
      code span.ot { color: #007020; } /* Other */
      code span.pp { color: #bc7a00; } /* Preprocessor */
      code span.sc { color: #4070a0; } /* SpecialChar */
      code span.ss { color: #bb6688; } /* SpecialString */
      code span.st { color: #4070a0; } /* String */
      code span.va { color: #19177c; } /* Variable */
      code span.vs { color: #4070a0; } /* VerbatimString */
      code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
      .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    </style>
    <!--<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>-->
    <script>document.addEventListener("DOMContentLoaded", function () {
    var mathElements = document.getElementsByClassName("math");
    var macros = [];
    /*
    const BATCH_SIZE = 100;
    var i = 0;
    while (i < mathElements.length) {
      const start = i;
      setTimeout(() => {
        for (var j = 0; j < BATCH_SIZE; j++) {
          const i = start + j;
          if (i >= mathElements.length) break;

          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }, 0);
      i += BATCH_SIZE;
    }
    */
    
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
      katex.render(texText.data, mathElements[i], {
        displayMode: mathElements[i].classList.contains('display'),
        throwOnError: false,
        macros: macros,
        fleqn: false
      });
    }}
    
    });
    </script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" />
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <h1>Single Page View</h1>

    
<small>This is an automatically generated page containing everything in a single view.</small>



<article>
<h2>Table of Contents</h2>
<nav class="toc">
                <ol>
                    
                    <li><a href="#comp4500-advanced-algorithms-data-structures">COMP4500 — Advanced Algorithms & Data Structures</a>
            		</li>

                    <li><a href="#introduction-and-background">Introduction and Background</a>
            
                <ol>
                    
                    <li><a href="#motivation">Motivation</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#running-time-analysis-and-asymptotic-notation">Running Time Analysis and Asymptotic Notation</a>
            
                <ol>
                    
                    <li><a href="#asymptotic-notation">Asymptotic Notation</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#lecture-10-randomised-algorithms">Lecture 10 — Randomised Algorithms</a>
            
                <ol>
                    
                    <li><a href="#average-case-analysis">Average case analysis</a>
            		</li>

                    <li><a href="#randomised-algorithms">Randomised algorithms</a>
            		</li>

                    <li><a href="#quicksort">Quicksort</a>
            		</li>

                    <li><a href="#biased-random">Biased random</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#lecture-11-revision">Lecture 11 — Revision</a>
            
                <ol>
                    
                    <li><a href="#topics">Topics</a>
            		</li>

                    <li><a href="#exam">Exam</a>
            		</li>

                    <li><a href="#strategy">Strategy</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#recursive-functions">Recursive functions</a>
            
                <ol>
                    
                    <li><a href="#recurrences">Recurrences</a>
            		</li>

                    <li><a href="#substitution">Substitution</a>
            		</li>

                    <li><a href="#iteration">Iteration</a>
            		</li>

                    <li><a href="#master-method">Master method</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#lecture-3-graphs">Lecture 3 — Graphs</a>
            
                <ol>
                    
                    <li><a href="#undirected-graph">Undirected graph</a>
            		</li>

                    <li><a href="#directed-graph">Directed graph</a>
            		</li>

                    <li><a href="#weights">Weights</a>
            		</li>

                    <li><a href="#terminology">Terminology</a>
            		</li>

                    <li><a href="#types-of-graphs">Types of graphs</a>
            		</li>

                    <li><a href="#representations">Representations</a>
            		</li>

                    <li><a href="#graph-traversal-algorithms">Graph traversal algorithms</a>
            		</li>

                    <li><a href="#topological-sort">Topological sort</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#lecture-4-graphs-2">Lecture 4 — Graphs 2</a>
            
                <ol>
                    
                    <li><a href="#minimum-spanning-tree">Minimum Spanning Tree</a>
            		</li>

                    <li><a href="#generic-constructive-algorithm">Generic Constructive Algorithm</a>
            		</li>

                    <li><a href="#prims-algorithm">Prim’s Algorithm</a>
            		</li>

                    <li><a href="#kruskals-algorithm">Kruskal’s Algorithm</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#lecture-5-graph-algorithms-2">Lecture 5 — Graph Algorithms 2</a>
            
                <ol>
                    
                    <li><a href="#single-source-shortest-path">Single source shortest path</a>
            		</li>

                    <li><a href="#non-negative-weights">Non-negative weights</a>
            		</li>

                    <li><a href="#dijkstras-algorithm">Dijkstra’s algorithm</a>
            		</li>

                    <li><a href="#negative-weights">Negative weights</a>
            		</li>

                    <li><a href="#bellman-ford">Bellman-Ford</a>
            		</li>

                    <li><a href="#priority-first-search">Priority first search</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#lecture-6-dynamic-programming">Lecture 6 — Dynamic Programming</a>
            
                <ol>
                    
                    <li><a href="#fibonacci-numbers">Fibonacci numbers</a>
            		</li>

                    <li><a href="#general-principle">General principle</a>
            		</li>

                    <li><a href="#longest-common-subsequence">Longest common subsequence</a>
            		</li>

                    <li><a href="#matrix-chain-multiplication">Matrix chain multiplication</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#lecture-7-dynamic-programming-2">Lecture 7 — Dynamic Programming 2</a>
            
                <ol>
                    
                    <li><a href="#all-pairs-shortest-paths">All-pairs shortest paths</a>
            		</li>

                    <li><a href="#recursive-formulation">Recursive formulation</a>
            		</li>

                    <li><a href="#floyd-warshall">Floyd-Warshall</a>
            		</li>

                    <li><a href="#johnsons-algorithm">Johnson’s algorithm</a>
            		</li>

                    <li><a href="#dp-vs-greedy-algorithms">DP vs greedy algorithms</a>
            		</li>

                    <li><a href="#activity-selection">Activity selection</a>
            		</li>

                    <li><a href="#knapsack-problem">Knapsack problem</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#lecture-8-amortised-analysis">Lecture 8 — Amortised Analysis</a>
            
                <ol>
                    
                    <li><a href="#amortised-analysis">Amortised analysis</a>
            		</li>

                    <li><a href="#stack">Stack</a>
            		</li>

                    <li><a href="#binary-counter">Binary counter</a>
            		</li>

                    <li><a href="#array-resizing">Array resizing</a>
            		</li>

                    <li><a href="#potential-method-1">Potential method</a>
            		</li>
                </ol>
            		</li>

                    <li><a href="#lecture-9-computational-complexity">Lecture 9 — Computational Complexity</a>
            
                <ol>
                    
                    <li><a href="#polynomial-time">Polynomial time</a>
            		</li>

                    <li><a href="#examples">Examples</a>
            		</li>

                    <li><a href="#background">Background</a>
            		</li>

                    <li><a href="#nondeterministic-polynomial-np">Nondeterministic polynomial (NP)</a>
            		</li>

                    <li><a href="#np-hard">NP-Hard</a>
            		</li>

                    <li><a href="#np-complete">NP-Complete</a>
            		</li>

                    <li><a href="#overview">Overview</a>
            		</li>

                    <li><a href="#approximations">Approximations</a>
            		</li>
                </ol>
            		</li>
                </ol>
            </nav>
</article>

<h1>Lecture 1</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 1 — Introduction</h1>
<p class="author">Kenton Lam</p>
</header>
-->


<h1 id="comp4500-advanced-algorithms-data-structures">COMP4500 — Advanced Algorithms &amp; Data Structures</h1>
<p>Lectured by Ahmad Abdel-Hafez. Currently works full-time as a data scientist. First time teaching at UQ. Not a mathematician.</p>
<h1 id="introduction-and-background">Introduction and Background</h1>
<h2 id="motivation">Motivation</h2>
<ul>
<li><em>Creating</em> more efficient algorithms.</li>
<li><em>Justifying</em> choice of algorithms using theory.</li>
<li><em>Improving</em> problem solving skills.</li>
<li>This is a prerequisite for a job at Google, Amazon, Oracle, etc.</li>
</ul>
<h3 id="research-areas">Research Areas</h3>
<p>These areas are currently being researched and developed:</p>
<ul>
<li>distributed/parallel algorithms,</li>
<li>neural networks/pattern recognition,</li>
<li>bioinformatics, and</li>
<li>quantum computing.</li>
</ul>
<h3 id="outline">Outline</h3>
<ul>
<li>Background and asymptotic notation.</li>
<li>Recurrences, divide and conquer.</li>
<li>Graph algorithms (3 weeks).</li>
<li>Dynamic programming (2 weeks). <strong>Assignment 1 published.</strong></li>
<li><strong>Midsemester exam.</strong></li>
<li><strong>Assignment 2 published.</strong></li>
<li>Greedy algorithms.</li>
<li>Amortised analysis.</li>
<li>Complexity classes.</li>
<li>Randomised algorithms.</li>
</ul>
<h3 id="other">Other</h3>
<ul>
<li>Textbook is Cormen at al. (CLRS).</li>
<li>Tutorials start in Week 2.</li>
<li>Piazza will be used.</li>
<li>Consultation with lecturer can be organised by email.</li>
</ul>
<h1 id="running-time-analysis-and-asymptotic-notation">Running Time Analysis and Asymptotic Notation</h1>
<ul>
<li><p>An <strong>algorithm</strong> is a well-defined computation procedure which takes some values as <em>input</em> and produces some value as <em>output</em>.</p></li>
<li><p>As an example, <em>insertion sort</em> is an algorithm taking an array as input and returning a sorted array. It works by building the sorted array from left to right by inserting the next value in its sorted position.</p></li>
<li><p><em>Loop invariants</em> can be used to prove an algorithm is correct.</p></li>
<li><p><strong>Execution time</strong> depends on input size and the input itself. Generally, we want an upper bound on the execution time.</p>
<ul>
<li>There are <em>worst</em>, <em>average</em>, and <em>best</em> case execution times. Average case is the average over all inputs, weighted by the probability of that input.</li>
</ul></li>
<li><p>For example, insertion sort requires <span class="math inline">n(n-1)/2</span> comparisons in the worst case and <span class="math inline">n-1</span> in the best case.</p></li>
</ul>
<h2 id="asymptotic-notation">Asymptotic Notation</h2>
<ul>
<li><p>For sufficiently large <span class="math inline">n</span>, the constants are negligible.</p></li>
<li><p>Example: <span class="math inline">2n^2 \in O(n^3 - n^2)</span>.</p></li>
<li><p>Theorems:</p>
<ul>
<li>If <span class="math inline">f \in O(g)</span>, then <span class="math inline">g + f \in \Theta(g)</span>. If <span class="math inline">f</span> grows no larger than <span class="math inline">g</span>, it doesn’t affect the big-<span class="math inline">\Theta</span> bound of <span class="math inline">g</span>.</li>
<li>If <span class="math inline">k &gt; 0</span>, then <span class="math inline">k n^a \in \Theta(n^a)</span>.</li>
<li>If <span class="math inline">k &gt; 0</span> and <span class="math inline">0 \le a \le b</span>, then <span class="math inline">k n^a \in O(n^b)</span>.</li>
</ul></li>
<li><p><span class="math inline">f</span> is asymptotically non-negative if <span class="math inline">f(n) \ge 0</span> for <span class="math inline">n \ge n_0</span>.</p></li>
<li><p>Theorem: If <span class="math inline">f, g</span> are asymptotically non-negative and <span class="math inline">\lim_{n \to \infty} f(n)/g(n) = c</span>, then:</p>
<ul>
<li><span class="math inline">f(n) \in O(g(n))</span> if <span class="math inline">c &lt; \infty</span>,</li>
<li><span class="math inline">f(n) \in \Theta(g(n))</span> if <span class="math inline">0 &lt; c &lt; \infty</span>, and</li>
<li>$f(n) (g(n)) $ if <span class="math inline">c &gt; 0</span>.</li>
</ul></li>
</ul><h1>Lecture 10</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 10 — Randomised Algorithms</h1>
<p class="author">Kenton Lam</p>
<p class="date">2020-11-08</p>
</header>
-->


<h1 id="lecture-10-randomised-algorithms">Lecture 10 — Randomised Algorithms</h1>
<p>We will talk about probabilistic analysis and randomised algorithms, with a focus on quicksort.</p>
<h2 id="average-case-analysis">Average case analysis</h2>
<p>Although we focus on worst-case complexities, there are also best case and average cases. The average case is the sum of each possible running time weighted by its probability. <span class="math display">
\begin{aligned}
T_{\text{worst}}(n) &amp;= \max_{|x|=n}T(n) \\ 
T_{\text{best}}(n) &amp;= \min_{|x|=n}T(n) \\ 
T_{\text{average}}(n) &amp;= \sum_{|x|=n}T(x)P(x)\\ 
\end{aligned}
</span></p>
<h3 id="hire-assistant">Hire assistant</h3>
<p>Consider an algorithm which needs to hire candidates from a list and performs a hire when the current candidate is better than the current hire. In pseudocode,</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">def</span> hire_assistant(n):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>    best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>        interview(candidate[j]) <span class="co"># cost c_i</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>        <span class="cf">if</span> candidate[j] <span class="op">&gt;</span> best:</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>            best <span class="op">=</span> j</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>            hire(candidate[j])  <span class="co"># cost c_h</span></span></code></pre></div>
<p>Let <span class="math inline">n</span> be the total number of candidates and <span class="math inline">m</span> be the number of hires made. The actual cost is <span class="math inline">nc_i + mc_h</span> and the worst case is if we hire everyone so <span class="math inline">n=m</span> and the cost is <span class="math inline">n(c_i+c_h)</span>. What is the average case?</p>
<p>We need to consider the probability of hiring the <span class="math inline">j</span>-th candidate. Assume the candidates are in random order and so any of the first <span class="math inline">j</span> candidates are equally likely to be the best. The probability the <span class="math inline">j</span>-th candidate is the best of the first <span class="math inline">j</span> candidates is <span class="math inline">1/j</span>.</p>
<p>Thus, the average cost of hiring candidates is <span class="math display">
c_h\sum_{j=1}^n \frac 1 j = c_h \ln n + O(1)
</span> which is much better than the worst-case of <span class="math inline">n c_h</span>.</p>
<h2 id="randomised-algorithms">Randomised algorithms</h2>
<p>The algorithm above is a deterministic algorithm.</p>
<p>A <strong>randomised algorithm</strong> is one where its behaviour is determined by both its inputs and some random number generator.</p>
<ul>
<li>For deterministic algorithms, we can calculate an average running time from the distribution of inputs.</li>
<li>For non-deterministic (randomised) algorithms, we need to calculate the <em>expected</em> running time without being to make an assumption on the probability distribution of the inputs.</li>
</ul>
<h3 id="randomly-permuting-arrays">Randomly permuting arrays</h3>
<p>How can we randomise the hire assistant algorithm? We want a uniform random permutation, so a <span class="math inline">1/n!</span> chance for each permutation of the <span class="math inline">n</span> elements in <span class="math inline">A</span>.</p>
<p>A simple way to do this is the algorithm below.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="kw">def</span> permute_by_sort(A):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>    n <span class="op">=</span> <span class="bu">len</span>(A)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>    P <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> n</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>        P[i] <span class="op">=</span> randint(<span class="dv">1</span>, n<span class="op">**</span><span class="dv">3</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>    <span class="co"># sort A using P as the keys</span></span></code></pre></div>
<p>This produces a unique random permutation, so long as the chosen keys are unique. The probability of keys being unique is <span class="math display">
\frac {n^3}{n^3} \times \frac{n^3-1}{n^3} \times \frac{n^3 - 2}{n^3}\times \cdots \times \frac{n^3-n}{n^3} \ge \left( 1 - \frac 1 {n^2} \right)^n \ge 1 - \frac 1 n.
</span> This is reasonably non-zero so is not good enough. In fact, this gets closer to <span class="math inline">1</span> as <span class="math inline">n</span> becomes large.</p>
<p>Alternatively, we can permute it by randomising in-place.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="kw">def</span> randomise_in_place(A):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>    n <span class="op">=</span> <span class="bu">len</span>(A)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>        <span class="co"># randomly swap i with an element to the right</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a>        swap(A[i], A[randint(i, n)]) </span></code></pre></div>
<p>A <span class="math inline">k</span>-permutation of a set of <span class="math inline">n</span> elements is a sequence containing <span class="math inline">k</span> of the <span class="math inline">n</span> elements. The invariant at <span class="math inline">i</span> is that <span class="math inline">A[1..i]</span> contains any <span class="math inline">i</span>-permutation of <span class="math inline">A</span> with probability <span class="math inline">(n-i)!/n!</span>. Before the first element, <span class="math inline">A[1..0]</span> contains a <span class="math inline">0</span>-permutation with probability <span class="math inline">1</span> (trivially).</p>
<p>Assume that the invariant holds at <span class="math inline">i-1</span>. We must show that the invariant holds at <span class="math inline">i</span>.</p>
<figure>
<img src="/assets/image-20201108170034381.png" alt="" /><figcaption>image-20201108170034381</figcaption>
</figure>
<p>At the termination of the algorithm, <span class="math inline">A[1..n]</span> will contain any <span class="math inline">n</span>-permutation with probability <span class="math display">
\frac{(n-n)!}{n!}=\frac 1 {n!},
</span> so each of the <span class="math inline">1/n!</span> possible permutations is equally likely.</p>
<h2 id="quicksort">Quicksort</h2>
<p>Quicksort is great because it’s “pretty fast” “most of the time”. It preprocesses the array into partitions based of less than or greater than some pivot value. In pseudocode:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="kw">def</span> quicksort(A, p, r):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>    <span class="cf">if</span> p <span class="op">&lt;</span> r:</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a>        q <span class="op">=</span> partition(A, p, r)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a>        quicksort(A, p, q<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>        quicksort(A, q<span class="op">+</span><span class="dv">1</span>, r)</span></code></pre></div>
<p>In contrast, mergesort postprocesses sorted subarrays into a larger sorted array.</p>
<p>The algorithms are very similar but differ in how they work with the recursive calls.</p>
<p>A visual representation of the partition algorithm is below.</p>
<figure>
<img src="/assets/image-20201108171447197.png" alt="" /><figcaption>image-20201108171447197</figcaption>
</figure>
<p>The performance of quicksort depends very much on the element chosen as the pivot. The best and average cases are <span class="math inline">\Theta(n \log n)</span> but the worst case is terrible at <span class="math inline">\Theta(n^2)</span>. With the partitioning scheme above, the worst case occurs if the array is already sorted or reverse sorted. This is actually where insertion sort has its best case.</p>
<p>How can we do this average case analysis? Later on, how can we randomise it so we get expected time complexity of <span class="math inline">n \log n</span> always?</p>
<p>There are some cases with different performance: <span class="math display">
\begin{aligned}
q=\left\lfloor\frac{p+q}2\right\rfloor &amp;\implies T(n) = 2T(n/2) + \Theta(n) &amp; \in \Theta(n \log n) \\ 
q=p+\frac{r-p}c &amp;\implies T(n) = T(n/c) + T((c-1)n/c) + \Theta(n) &amp;\in \Theta(n \log n) \\ 
q=p &amp;\implies T(n) = T(n-1) + T(0) + \Theta(n) &amp;\in \Theta(n^2)
\end{aligned}
</span> Interestingly, when there is a proportional split as in the second case, the running time is still (asymptotically) optimal.</p>
<h3 id="analysis-of-quicksort">Analysis of quicksort</h3>
<p>We assume that each permutation is equally likely. We need to consider the total number of comparisons done by partition over <em>all</em> recursive calls of quicksort. Label the elements of <span class="math inline">A</span> as <span class="math inline">z_1, \ldots, z_n</span> in sorted order. Let <span class="math inline">Z_{ij}</span> be the <span class="math inline">i</span>-th to <span class="math inline">j</span>-th elements in sorted order.</p>
<p>Consider an array of <span class="math inline">1..10</span> in any order and assume the first pivot is 4. The array would be partitioned into <span class="math inline">\{1, 2, 3\}</span> and <span class="math inline">\{5, 6, 7, 8, 9, 10\}</span>. In the partitioning, we compare 4 to every other element. At this point, nothing in the left set will ever be compared to an element of the right set.</p>
<p>For any elements <span class="math inline">z_i</span> and <span class="math inline">z_j</span>, once a pivot <span class="math inline">x</span> is chosen such that <span class="math inline">z_i &lt; x &lt; z_j</span>, then <span class="math inline">z_i</span> will never be compared to <span class="math inline">z_j</span> in the future.</p>
<p>If <span class="math inline">z_i</span> is chosen as a pivot before an other element in <span class="math inline">Z_{ij}</span>, it will be compared to every other element in <span class="math inline">Z_{ij}</span>. Similarly for <span class="math inline">z_j</span>. This, <span class="math inline">z_i</span> and <span class="math inline">z_j</span> are compared if and only if the first element chosen as pivot from <span class="math inline">Z_{ij}</span> is either <span class="math inline">z_i</span> or <span class="math inline">z_j</span>. Any element in <span class="math inline">Z_{ij}</span> is equally likely and <span class="math inline">Z_{ij}</span> has <span class="math inline">j-i+1</span> equally likely elements.</p>
<p>Each pair of elements is compared at most once because in partition, elements are compared with the pivot only at most once, and an element is only used as a pivot in at most one partition step. <span class="math display">
\begin{aligned}
P(z_i \text{ compared to }z_j)&amp;=P(z_i\text{ or }z_j \text{ is first pivot chosen from } Z_{ij}) \\ 
&amp;= \frac {1}{j-i+1} + \frac {1}{j-i+1} \\ 
&amp;= \frac {2}{j-i+1}
\end{aligned}
</span> Now, we just need to sum over all pairs of possible comparisons. Then, <span class="math display">
\begin{aligned}
\sum_{i=1}^{n-1}\sum_{j=i+1}^nP(z_i \text{ compared to }z_j)  
&amp;= \sum_{i=1}^{n-1}\sum_{j=i+1}^n\frac 2 {j-i+1} \\ 
&amp;= \sum_{i=1}^{n-1}\sum_{k=1}^{n-i}\frac 2 {k+1} \\ 
&amp;&lt;  \sum_{i=1}^{n-1}\sum_{k=1}^{n}\frac 2 {k} \\ 
&amp;\in \sum_{i=1}^{n-1}O(\log n) \in O(n \log n)
\end{aligned}
</span> This is assuming the inputs are evenly distributed, but this is not likely in practice. Usually, we will get given arrays which are almost sorted.</p>
<p>We can randomise it which ensures that the input arrays are all evenly distributed by randomly permuting the array before sorting it. Random permuting can be done in <span class="math inline">\Theta(n)</span>. In fact, we don’t even need to permute the entire array. We can just randomly choose a pivot from a random location.</p>
<p>Either method will give us <strong>expected running time</strong> of <span class="math inline">\Theta(n \log n)</span> and the worst case will be unlikely.</p>
<h2 id="biased-random">Biased random</h2>
<p>In random algorithms, we typically assume we have access to some fair coin via RNG. In practice, we may not have a fair coin. We might have a biased coin with probability <span class="math inline">p \ne 1/2</span>. If all we have is a biased coin, how can we implement a fair coin which returns 0 or 1 with equal probability?</p>
<p>Suppose we flip the coin twice. The probability of both 0 is <span class="math inline">p^2</span>. The probability of 0 then 1 or 1 then 0 is <span class="math inline">p(1-p)</span> and the probability of both 1 is <span class="math inline">(1-p)^2</span>. Then, the probability that <span class="math inline">a=0</span> <em>given</em> <span class="math inline">a \ne b</span> is <span class="math inline">1/2</span>.</p>
<p>We can continue until we get 0 and 1 or 1 and 0, returning an arbitrary one when they differ.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="kw">def</span> random():</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a>    a <span class="op">=</span> biased_random()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>    b <span class="op">=</span> biased_random()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a>    <span class="cf">while</span> a <span class="op">==</span> b:</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>        a <span class="op">=</span> biased_random()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a>        b <span class="op">=</span> biased_random()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a>    <span class="cf">return</span> a</span></code></pre></div>
<p>The running time depends on <span class="math inline">p</span> and we have <em>almost certain</em> termination.</p>
<p>Let <span class="math inline">\alpha=2p(1-p)</span> be the probability that <span class="math inline">a \ne b</span>. The probability of terminating after <span class="math inline">0</span> iterations is just <span class="math inline">\alpha</span>. After once, it is <span class="math inline">\alpha(1-\alpha)</span>. Continuing, the probability of terminating after exactly <span class="math inline">i</span> iterations is <span class="math display">
(1-\alpha)^i\alpha.
</span> The expected number of iterations is the sum of <span class="math inline">i</span> multiplied by the probability of each, <span class="math display">
\begin{aligned}
\sum_{i=0}^\infty i ((1-\alpha)^i\alpha) &amp;= \alpha \sum_{i=0}^\infty i (1-\alpha)^i \\ 
&amp;= \frac \alpha{(1-(1-\alpha))^2} \\ 
&amp;= \frac \alpha{\alpha^2} = \frac 1 \alpha
\end{aligned}
</span> This directly gives us the expected running time.</p><h1>Lecture 11</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 11 — Revision</h1>
<p class="author">Kenton Lam</p>
<p class="date">2020-11-08</p>
</header>
-->


<h1 id="lecture-11-revision">Lecture 11 — Revision</h1>
<p>The goal of this course has been to analyse, critique, design, and implement advanced data structures and algorithms. When faced with a problem, how can we convert it to a solvable problem in an efficient way?</p>
<p>Questions will require some amount of thinking and analysing instead of directly application.</p>
<h2 id="topics">Topics</h2>
<ul>
<li>Measuring and comparing efficiency</li>
<li>Representing problems as graphs</li>
<li>Dynamic programming</li>
<li>Greedy algorithms</li>
<li>Probabilistic analysis and randomised algorithms</li>
</ul>
<p>We also learned about complexity classes, so we can</p>
<ul>
<li>understand the limits of our knowledge (P vs NP), and</li>
<li>know types of problems which may not have an efficient solution.</li>
</ul>
<h2 id="exam">Exam</h2>
<p>The exam will be <strong>2 hours + 30 minutes</strong>, with the 30 minutes for preparing and uploading the exam document. Some general directions are:</p>
<ul>
<li><p>Answer all questions.</p></li>
<li><p>100 marks.</p></li>
<li><p>Worth 50% or 60%, whichever results in the higher final result.</p></li>
<li><p>Open book.</p></li>
<li><p><strong>5 questions.</strong></p></li>
<li><p>Available for 2 hours from 4pm.</p></li>
</ul>
<p>There is a penalty for late submissions of 20% for 1 to 15 minutes late, 50% for up to 30 minutes, and 100% for more than 30 minutes late. The penalty is applied on the marks received, not on the total available marks.</p>
<h2 id="strategy">Strategy</h2>
<ul>
<li>Start with easiest questions (it should be easier than the midsem).</li>
<li>Don’t waste time searching online for answers.</li>
<li><strong>Read answers carefully and be comprehensive with answers.</strong></li>
<li>Even if you do not know the answers, <em>try</em>.</li>
<li>Nothing written is useless.</li>
<li>Exam is designed to test problem solving. Do not share answers.</li>
</ul><h1>Lecture 2</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 2 — Recurrences</h1>
<p class="author">Kenton Lam</p>
</header>
-->


<h1 id="recursive-functions">Recursive functions</h1>
<p>There are three strategies we can use:</p>
<ul>
<li>substitution, where we guess an answer and prove it satisfies the recurrence,</li>
<li>iteration, expanding into a sum and evaluating, and</li>
<li>master theorem which covers some particular cases.</li>
</ul>
<h3 id="divide-and-conquer-algorithms">Divide and conquer algorithms</h3>
<p>Recall merge sort, defined as <span class="math inline">\texttt{merge_sort}(A, p, r)</span>. This also uses a subroutine <span class="math inline">\texttt{merge}(A, p, q, r)</span> which merges the subarrays <span class="math inline">A[p,q]</span> and <span class="math inline">A[q+1, r]</span> into <span class="math inline">A[p, r]</span> (this algorithm is <span class="math inline">\Theta(n)</span> where <span class="math inline">n=r-p+1</span>).</p>
<p>The time complexity of merge sort is <span class="math inline">T(n) = 2T(n/2) + f(n)</span> with <span class="math inline">f(n) \in \Theta(n)</span>.</p>
<h2 id="recurrences">Recurrences</h2>
<p>To be well-defined, a recurrence relation needs a base case and recursive case(s) which converge towards that base case. Sometimes, we omit the base case and implicitly take it as <span class="math inline">T(n) = \Theta(1)</span> for <span class="math inline">n \le c</span>. This does not have a significant effect on the running time of the base case.</p>
<p>Consider a generic divide and conquer algorithm which</p>
<ul>
<li>takes input of size <span class="math inline">n</span>,</li>
<li>breaks it into <span class="math inline">a</span> parts, each of size <span class="math inline">n/b</span>,</li>
<li>takes <span class="math inline">D(n)</span> time to divide the problem in this way, and</li>
<li>takes <span class="math inline">C(n)</span> time to combine the subproblem results.</li>
</ul>
<p>This results in the following recurrence: <span class="math display">
T(n) = \begin{cases}
aT(n/b) + D(n) + C(n), &amp; n \ge c, \\ 
\Theta(1), &amp; n \le c.
\end{cases}
</span></p>
<h2 id="substitution">Substitution</h2>
<p><strong>Example:</strong> Consider <span class="math display">
T(n) = \begin{cases}
2, &amp; n=2,\\
2T(n/2) + n, &amp;n=2^k,
\end{cases}
</span> and we guess that <span class="math display">
T(n) = n \log_2 n.
</span> The base case is that <span class="math inline">T(2) = 2 = 2 \log_2(2)</span>. The inductive step is <span class="math display">
\begin{aligned}
T(n) = 2T(n/2)+n &amp;= 2(n/2)\log_2(n/2) + n \\ 
&amp;= n(\log_2 n - \log_22) + n \\ 
&amp;= n \log_2 n
\end{aligned}
</span> which is what we wanted to show.</p>
<p><strong>Example:</strong> As a more complicated example using <span class="math display">
T(n) =\begin{cases}
1, &amp; n=1, \\ 
2T(\lfloor n/2\rfloor) + n, &amp; n&gt;1.
\end{cases}
</span> We notice it resembles the earlier example and we guess that <span class="math inline">T(n) \in O(n \lg n)</span>. We need to prove this using the definition.</p>
<p>First, we need an <span class="math inline">n_0</span>. We can see that <span class="math inline">n_0=1</span> doesn’t work because <span class="math inline">T(1) = 1</span> which is not less than <span class="math inline">0</span>. Suppose <span class="math inline">n_0=2</span>, then for the cases which depend directly on <span class="math inline">T(1)</span>, we have <span class="math display">
\begin{aligned}
T(2)&amp; = 2T(1) + 2 = 4 \le c2 \lg 2\\
T(3)&amp; = 2T(1) + 3 = 5 \le c 2 \lg 2\\
\end{aligned}
</span> for some <span class="math inline">c \ge 2</span>.</p>
<p>Moving onto the induction, we assume that <span class="math inline">T(n) \le c n \lg n</span> for <span class="math inline">\lfloor n/2\rfloor</span> and we want prove this for the case of <span class="math inline">n</span>. Substituting, <span class="math display">
\begin{aligned}
T(n) = 2T(\lfloor n/2\rfloor) + n  
&amp;\le 2(c \lfloor n/2\rfloor \lg \lfloor n/2\rfloor) + n \\ 
&amp;\le c  n \lg \lfloor n/2\rfloor + n \\ 
&amp;=  c  n (\lg n - \lg 2) + n \\  
&amp;=  cn\lg n - cn\lg 2 + n \\ 
&amp;\le cn \lg n
\end{aligned}
</span> assuming <span class="math inline">c \ge 1</span> and <span class="math inline">n \ge 0</span>.</p>
<p><strong>Example:</strong> Guessing does not always work in this way. Consider, <span class="math display">
T(n) = T(\lfloor n/2 \rfloor) + T(\lceil n/2 \rceil) + 1
</span> and we guess that <span class="math inline">T(n) = O(n)</span> (which is actually correct). Trying to prove the inductive step, we get <span class="math display">
\begin{aligned}
T(n) = T(\lfloor n/2 \rfloor) + T(\lceil n/2 \rceil) + 1 
&amp;\le c(\lfloor n/2 \rfloor) + c(\lceil n/2 \rceil) + 1  \\ 
&amp;=cn+1 
\end{aligned}
</span> which is not <span class="math inline">\le cn</span>, so we cannot prove it this way! We can solve this problem by strengthening the guess_, via subtracting a lower order term. Specifically, we assume that <span class="math inline">T(n) \le cn-b</span> for <span class="math inline">b&gt;0</span>. This does not affect the asymptotic bound but with this stronger assumption, we are able to probe a stronger result.</p>
<p><strong>Note:</strong> In the inductive step, we need to be very precise. We need to prove that <span class="math inline">T(n) \le cn</span> very carefully, something like <span class="math inline">cn + n = O(n)</span> is not good enough.</p>
<h4 id="change-of-variables">Change of variables</h4>
<p>Consider <span class="math display">
T(n) = 2 T(\lfloor \sqrt n \rfloor) + \lg n.
</span> This looks hard, but one trick we can use for some more unfamiliar cases is changing the variables. We try <span class="math inline">m = \lg n</span> which means <span class="math inline">n = 2^m</span>. Making the change, <span class="math display">
T(2^m)=2T(2^{m/2}) + m \iff S(m) = 2S(m/2) + m
</span> where <span class="math inline">S(m) = T^(2^m)</span>. The <span class="math inline">S</span> expression looks familiar and we know the solution is <span class="math inline">S(m) \in \Theta(m \lg m)</span>. Changing back gives us <span class="math display">
T(n) = T(2^{m}) = S(m) = \Theta(m \lg m) = \Theta(\lg n \lg(\lg n)).
</span></p>
<h2 id="iteration">Iteration</h2>
<h2 id="master-method">Master method</h2>
<p>Suppose <span class="math inline">T</span> is of the form <span class="math display">
T(n) = aT(n/b) + f(n)
</span> where <span class="math inline">n/b</span> can also have a ceiling or floor. Then, we need to compare the work at each step, <span class="math inline">f(n)</span> with how many calls there are. Specifically,</p>
<ul>
<li><ol type="1">
<li>if <span class="math inline">n^{\log_b a}</span> is polynomially <em>larger</em> than <span class="math inline">f(n)</span>, then <span class="math inline">T(n) \in \Theta(n^{\log_ba})</span>,</li>
</ol></li>
<li><ol start="2" type="1">
<li>if <span class="math inline">n^{\log_b a}</span> is the same asymptotic <em>tight</em> bound as <span class="math inline">f(n)</span>, then <span class="math inline">T(n) \in \Theta(n^{\log_ba}\lg n)</span>, and</li>
</ol></li>
<li><ol start="3" type="1">
<li>if <span class="math inline">f(n)</span> is polynomially <em>larger</em> than <span class="math inline">n^{\log_ba}</span> and <span class="math inline">f(n)</span> is regular, then <span class="math inline">T(n) \in \Theta(f(n))</span>.</li>
</ol></li>
</ul>
<p>We say a function <span class="math inline">f</span> is <strong>polynomially larger than</strong> <span class="math inline">g</span> if <span class="math inline">f</span> is lower bounded by <span class="math inline">g</span> times some polynomial with degree <span class="math inline">\epsilon&gt;0</span>. That is, <span class="math display">
f(n) \in \Omega(g(n) \times n^\epsilon) \quad \text{where }\epsilon &gt; 0
</span> or equivalently, <span class="math display">
f(n) / g(n) \in \Omega(n^\epsilon).
</span> A function is <strong>regular</strong> if <span class="math inline">af(n/b)&lt; cf(n)</span> for <span class="math inline">c&lt;1</span>. This just ensures that the subproblems are decreasing in complexity.</p>
<p>Putting this together, we have these three cases:</p>
<ul>
<li><ol type="1">
<li><span class="math inline">f(n) \in O(n^{\log_{b}a-\epsilon})\implies T(n) \in \Theta(n^{\log_b a})</span>,</li>
</ol></li>
<li><ol start="2" type="1">
<li><span class="math inline">f(n) \in \Theta(n^{\log_ba})\implies T(n) \in \Theta(n^{\log_b a} \lg n)</span>, and</li>
</ol></li>
<li><ol start="3" type="1">
<li><span class="math inline">f(n) \in \Omega(n^{\log_ba+\epsilon})</span> and regularity <span class="math inline">\implies T(n) \in \Theta(f(n))</span>.</li>
</ol></li>
</ul>
<p><strong>Example:</strong> Consider merge sort from earlier with <span class="math inline">T(n) = 2 T(n/2) + f(n)</span> with <span class="math inline">f(n) \in \Theta(n)</span>. We compare <span class="math inline">f(n)</span> to <span class="math inline">n^{\log_22}=n</span> and we see they are asymptotically the same. As a result, this is case 2 and <span class="math inline">T(n) \in \Omega(n \lg n)</span>.</p><h1>Lecture 3</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 3 — Graphs</h1>
<p class="author">Kenton Lam</p>
<p class="date">2020-08-18</p>
</header>
-->


<h1 id="lecture-3-graphs">Lecture 3 — Graphs</h1>
<p>A <strong>graph</strong> is made up of <em>vertices</em> and <em>edges</em> between pairs of vertices.</p>
<figure>
<img src="/assets/image-20200818081442148.png" alt="" /><figcaption>image-20200818081442148</figcaption>
</figure>
<p>Graphs can be used to represent a range of scenarios, including maps, networks, brains, and program control flow.</p>
<p>A graph can be directed or undirected.</p>
<h2 id="undirected-graph">Undirected graph</h2>
<ul>
<li>No self-loops.</li>
<li><span class="math inline">(u, v) \in E(G)</span> means <span class="math inline">v</span> is <strong>adjacent</strong> to <span class="math inline">u</span> and this is symmetric.</li>
<li>We also say <span class="math inline">(u, v)</span> is <strong>incident</strong> <strong>on</strong> <span class="math inline">v</span> and <span class="math inline">u</span>.</li>
<li>The <strong>degree</strong> of a vertex is the number of adjacent edges.</li>
</ul>
<h2 id="directed-graph">Directed graph</h2>
<ul>
<li>Can have self-loops.</li>
<li><span class="math inline">(u, v) \in E(G)</span> means <span class="math inline">v</span> is <strong>adjacent</strong> to <span class="math inline">u</span> and this is not symmetric.</li>
<li>We also say <span class="math inline">(u, v)</span> is <strong>incident from</strong> <span class="math inline">u</span> and <strong>incident to</strong> <span class="math inline">v</span>.</li>
<li>The <strong>out-degree</strong> is the number of edges leaving it.</li>
<li>The <strong>in-degree</strong> is the number of edges entering it.</li>
<li>The <strong>degree</strong> is the sum of in- and out-degrees.</li>
</ul>
<p>Note that the maximum edges in a directed graph is <span class="math inline">n^2</span>, and <span class="math inline">n(n-1)/2</span> for an undirected graph.</p>
<h2 id="weights">Weights</h2>
<p>Another key property of graphs is weights. An edge (directed or undirected) can have weights. This can represent something like distance, similarity, or cost. In a social media friends network, this might be the number of mutual friends.</p>
<h2 id="terminology">Terminology</h2>
<p>A graph <span class="math inline">G</span> is:</p>
<ul>
<li>A <strong>path</strong> of length <span class="math inline">k</span> from <span class="math inline">v_0</span> to <span class="math inline">v_k</span> is a sequence <span class="math inline">\langle v_0, \ldots, v_k\rangle</span> such that <span class="math inline">(v_{i-1}, v_i) \in E</span> for <span class="math inline">i=1, \ldots, k</span>.</li>
<li><span class="math inline">u</span> is <strong>reachable</strong> from <span class="math inline">v</span> if there is any path from <span class="math inline">v</span> to <span class="math inline">u</span>.</li>
<li>A path is <strong>simple</strong> if all vertices in the path are distinct.</li>
<li>A path is a <strong>cycle</strong> if <span class="math inline">v_0=v_k</span> and <span class="math inline">k&gt;1</span> (i.e. not a self-loop).</li>
<li>A cycle is <strong>simple</strong> if it is distinct except for its ends.</li>
<li>A graph with no simple cycles is called <strong>acyclic</strong>.</li>
</ul>
<p>An undirected graph <span class="math inline">G</span> is</p>
<ul>
<li><strong>connected</strong> if every vertex is reachable from any other,</li>
<li>a <strong>forest</strong> if it is acyclic, and</li>
<li>a <strong>tree</strong> if it is a forest with only one connected component.</li>
</ul>
<p>Note that for an undirected graph with <span class="math inline">n</span> vertices, a tree must have <span class="math inline">n-1</span> edges, and a forest can have <span class="math inline">0</span> to <span class="math inline">n-1</span> vertices. In a connected undirected graph, the minimum edges is <span class="math inline">n-1</span> and the maximum is <span class="math inline">n(n-1)/2</span> because we must have connections and the maximum is an arithmetic series.</p>
<p>A directed graph <span class="math inline">G</span> is</p>
<ul>
<li><strong>strongly connected</strong> if any two vertices are reachable from each other.</li>
</ul>
<p>A graph <span class="math inline">G&#39;=(V&#39;, E&#39;)</span> is</p>
<ul>
<li>a <strong>subgraph</strong> of <span class="math inline">G=(V,E)</span> if <span class="math inline">V&#39; \subseteq V</span> and <span class="math inline">E&#39;\subseteq E</span>, and</li>
<li>a <strong>spanning subgraph</strong> if it is a subgraph and <span class="math inline">V&#39;=V</span>.</li>
</ul>
<p>The subgraph of <span class="math inline">G</span> which is <strong>induced by <span class="math inline">V&#39;</span></strong> is <span class="math inline">G&#39;=(V&#39;, E&#39;)</span> where <span class="math inline">E&#39;=\{(u, v) \in E : u \in V&#39; \wedge v \in V&#39;\}</span>. That is, it has all edges which start and end in <span class="math inline">V&#39;</span>.</p>
<h2 id="types-of-graphs">Types of graphs</h2>
<ul>
<li>Directed acyclic graph (DAG)</li>
<li>Connected graph</li>
<li>Trees</li>
<li>Lists can also be seen as simple graphs (see linked lists)</li>
</ul>
<h2 id="representations">Representations</h2>
<p>There are two main ways to represent graphs while programming:</p>
<ul>
<li>adjacency lists, and</li>
<li>adjacency matrices.</li>
</ul>
<p>Representing the sets of vertices and edges directly is often inefficient.</p>
<h3 id="adjacency-list">Adjacency list</h3>
<p>For undirected graphs,</p>
<ul>
<li>the worst-case space complexity is <span class="math inline">\Theta(v + \sum_{v}\operatorname{degree}(v))=\Theta(v+2e) \in \Theta(v+e)</span>,</li>
<li>the time complexity of <span class="math inline">\operatorname*{isAdjacentTo}(u, v)</span> is <span class="math inline">\Theta(v)</span>, and</li>
<li>the time complexity of listing all adjacent vertex pairs is <span class="math inline">\Theta(v+e)</span>.</li>
</ul>
<p>Similarly for directed graphs,</p>
<ul>
<li>the space complexity is <span class="math inline">\Theta(v+e)</span>,</li>
<li>the time complexity if <span class="math inline">\operatorname*{isAdjacentTo}</span> is <span class="math inline">\Theta(v)</span>, and</li>
<li>listing adjacent vertex pairs is <span class="math inline">\Theta(v+e)</span>.</li>
</ul>
<p>However, in this case, the bounds make use of <span class="math inline">\operatorname*{outDegree}</span> instead of just <span class="math inline">\operatorname*{degree}</span> in the derivation.</p>
<h3 id="adjacency-matrix">Adjacency matrix</h3>
<p>An adjacency matrix is a <span class="math inline">v \times v</span> matrix where the <span class="math inline">i,j</span>-th entry indicates if there is an edge from <span class="math inline">i</span> to <span class="math inline">j</span>.</p>
<p>For undirected graphs, the matrix will be symmetric, and</p>
<ul>
<li>the space complexity is <span class="math inline">\Theta(v^2)</span>,</li>
<li>the time complexity of <span class="math inline">\operatorname*{isAdjacentTo}</span> is <span class="math inline">\Theta(1)</span> (because we only check the specific cell), and</li>
<li>the time complexity of listing all adjacent vertex pairs is <span class="math inline">\Theta(v^2)</span>.</li>
</ul>
<p>Note that the optimal representation depends very much on the type of operations we expect to do and the properties of the graphs.</p>
<p>For a directed graph, the complexities are the same but the matrix may be asymmetric.</p>
<h3 id="comparison-of-adjacency-list-and-adjacency-matrix">Comparison of adjacency list and adjacency matrix</h3>
<p>The adjacency list is often more efficient if the graph is sparse, and matrix is better if the graph is dense.</p>
<h2 id="graph-traversal-algorithms">Graph traversal algorithms</h2>
<p>For an unweighted graph <span class="math inline">G</span>,</p>
<ul>
<li>the <strong>length of a path</strong> is the number of edges in that path,</li>
<li>the <strong>distance</strong> from <span class="math inline">u</span> to <span class="math inline">v</span> is the shortest path length from <span class="math inline">u</span> to <span class="math inline">v</span>.</li>
</ul>
<h3 id="breadth-first-search">Breadth-first search</h3>
<p><strong>Breadth-first search</strong> (BFS) takes an unweighted graph and a start vertex <span class="math inline">v_0</span>. It traverses vertices within <span class="math inline">G</span> in order of their distance from <span class="math inline">v_0</span>. It is able to find the shortest path from <span class="math inline">v_0</span> to every other vertex in the graph.</p>
<h4 id="implementation">Implementation</h4>
<p>This can be implemented using a queue (FIFO) data structure, which has operations enqueue and dequeue.</p>
<p>We augment the vertex type with distance, state, and parent fields. The parent fields will be used to obtain the shortest path.</p>
<h3 id="depth-first-search">Depth-first search</h3>
<p>Depth-first search (DFS) does not find the shortest path, but is often used as a subroutine in other algorithms.</p>
<p>It visits a vertex <span class="math inline">v</span> then visiting all unvisited adjacent vertices <span class="math inline">u</span> to <span class="math inline">v</span>.</p>
<p>It has complexity <span class="math inline">\Theta(v) + (\sum_{v}(\Theta(1) + \operatorname*{outDegree}(v)\Theta(1)))=\Theta(v+e)</span>.</p>
<h2 id="topological-sort">Topological sort</h2>
<p>To implement topological sort, we can DFS and add vertices to the end of the list when it has no further unvisited neighbours.</p><h1>Lecture 4</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 4 — Graphs 2</h1>
<p class="author">Kenton Lam</p>
<p class="date">2002-08-25</p>
</header>
-->


<h1 id="lecture-4-graphs-2">Lecture 4 — Graphs 2</h1>
<blockquote>
<p>The midsemester exam will be on 15th September.</p>
</blockquote>
<p>Recall graphs are used to represent a great many problems. They may be directed/undirected and weighted/unweighted.</p>
<p>They can be represented (most commonly) by an adjacency list or an adjacency matrix.</p>
<p>Two search methods are BFS and DFS. BFS is able to find shortest distances from a graph. DFS can be used as a subroutine.</p>
<h2 id="minimum-spanning-tree">Minimum Spanning Tree</h2>
<p>Suppose you are laying cable and want to connect some houses using the least amount of cable possible. This is exactly a minimum spanning tree problem.</p>
<p>Suppose we have a connected, undirected, weighted graph <span class="math inline">G=(V, E)</span> and weights <span class="math inline">w(u, v)</span> denoting the weight between <span class="math inline">u</span> and <span class="math inline">v</span>. We want to find an acyclic subset <span class="math inline">T \subseteq E</span> such that</p>
<ul>
<li>all vertices in <span class="math inline">G</span> are connected, and</li>
<li>the total weight of <span class="math inline">T</span>, <span class="math inline">\sum w(u, v)</span> is minimised.</li>
</ul>
<p>Specifically, <span class="math inline">T</span> is a <strong>tree</strong> (a connected acyclic subgraph of <span class="math inline">G</span>) that <strong>spans</strong> (contains all vertices in <span class="math inline">G</span>) and <strong>minimises</strong> the sum of edge weights.</p>
<h2 id="generic-constructive-algorithm">Generic Constructive Algorithm</h2>
<p>Start with an empty set for <span class="math inline">T</span> and while <span class="math inline">T</span> is not a spanning tree, find an edge which can be added to <span class="math inline">T</span> (i.e. does not create any cycles) and add it.</p>
<h2 id="prims-algorithm">Prim’s Algorithm</h2>
<p>Here, <span class="math inline">T</span> is always a tree. Start at any vertex (this will be our initial <span class="math inline">T</span>). Repeatedly add the least-weight edge leaving the constructed tree <span class="math inline">T</span>. For each edge leaving the adjacent vertex, we update that in a priority queue. The algorithm stops when <span class="math inline">T</span> is a spanning tree, i.e. contains every vertex from <span class="math inline">G</span>.</p>
<p>How can we find the least-weight edge leaving <span class="math inline">T</span>?</p>
<ul>
<li>Maintain a priority queue <span class="math inline">Q</span> containing vertices <span class="math inline">V-T</span>. For each vertex in this queue, its key is the minimum edge weight connecting it to <span class="math inline">T</span>. Additionally, its parent is the vertex adjacent along this least-weight edge.</li>
<li>Recall that a priority queue’s operations are insert, extractMin, and decreaseKey.</li>
</ul>
<h2 id="kruskals-algorithm">Kruskal’s Algorithm</h2>
<p>Also used to find a minimum spanning tree. Here, <span class="math inline">T</span> is always a spanning acyclic subgraph, a <em>forest</em> of trees. Initially, <span class="math inline">T</span> is all vertices but no edges.</p>
<p>At each step, the least-weight edge connecting two trees in <span class="math inline">T</span> is added to the forest. We repeat this process until <span class="math inline">T</span> is connected.</p>
<p>The trees in <span class="math inline">T</span> are represented using a disjoint set data structure. Each set has a representative element from that set. Its operations are makeSet, findSet, and union. These disjoint sets are represented by rooted trees, and the representative element is the root of the tree. Each element in these trees stores a pointer to its parent (or itself if it is the root) and a rank (an upper bound on the node’s height in the tree).</p>
<ul>
<li><p>makeSet just creates a new single-node tree. Its parent will be the node itself and its rank is zero.</p></li>
<li><p>findSet returns the root of the tree. To make this fast, it collapses the path from (grand)children to point directly to the root.</p>
<figure>
<img src="/assets/image-20200825092403249.png" alt="" /><figcaption>image-20200825092403249</figcaption>
</figure></li>
<li><p>union works by linking the representative elements of its arguments. The subtree with greater rank is used as the combined representative element. If they have the same rank, choose arbitrarily but increment the final rank.</p>
<figure>
<img src="/assets/image-20200825092746558.png" alt="" /><figcaption>image-20200825092746558</figcaption>
</figure>
<figure>
<img src="/assets/image-20200825092759020.png" alt="" /><figcaption>image-20200825092759020</figcaption>
</figure></li>
</ul>
<h3 id="pseudocode">Pseudocode</h3>
<ul>
<li>Start by making disjoint sets for each vertex.</li>
<li>Order all edges ascending by weight.</li>
<li>For each edge <span class="math inline">(u, v)</span> in this sorted list:
<ul>
<li>If the set of <span class="math inline">u</span> is not the set of <span class="math inline">v</span>, add <span class="math inline">(u, v)</span> to <span class="math inline">T</span> and union the sets.</li>
</ul></li>
<li>After iterating through all edges, <span class="math inline">T</span> is our minimum spanning tree.</li>
</ul><h1>Lecture 5</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 5 — Graph Algorithms 2</h1>
<p class="author">Kenton Lam</p>
<p class="date">2020-09-08</p>
</header>
-->


<h1 id="lecture-5-graph-algorithms-2">Lecture 5 — Graph Algorithms 2</h1>
<p>Recall the shortest path is a sequence of vertices from a particular start to a particular end. The total weight of the path is the sum of all edge paths.</p>
<p>There are several types:</p>
<ul>
<li>Single pair: the shortest path between two given vertexes <span class="math inline">(u, v)</span>.</li>
<li>Single source: Given a vertex <span class="math inline">v</span>, find shortest paths to every other vertex.</li>
<li>Single destination: Given a destination <span class="math inline">v</span>, find shortest paths <em>from</em> every other vertex.</li>
<li>All pairs: Find the shortest path between every pair of vertices.</li>
</ul>
<h2 id="single-source-shortest-path">Single source shortest path</h2>
<p>Given a graph <span class="math inline">G</span> with weight function <span class="math inline">w</span> and source <span class="math inline">s</span>, for each <span class="math inline">v \in G.V</span>, we need to calculate</p>
<ul>
<li><span class="math inline">v.d</span>, its distance from <span class="math inline">s</span>, and</li>
<li><span class="math inline">v.\pi</span>, its predecessor from a shortest path from <span class="math inline">s</span> to <span class="math inline">v</span>.</li>
</ul>
<ol type="1">
<li>We initialise <span class="math inline">s.d=0</span> and <span class="math inline">v.d=\infty</span> else. The invariant here is that the shortest distance from <span class="math inline">s</span> to <span class="math inline">v</span> is less than or equal to <span class="math inline">v.d</span> less than or equal to <span class="math inline">\infty</span>.</li>
<li>Initialise all <span class="math inline">v.\pi = \textit{null}</span>.</li>
<li>Perform a relaxation on the distance estimates until we reach a solution.</li>
</ol>
<h3 id="relax">Relax</h3>
<p>This checks if we can reach <span class="math inline">v</span> (from <span class="math inline">s</span>) sooner by going through <span class="math inline">u</span>. If so, we reduce the distance and update the predecessor.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">def</span> relax(u, v, w):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>  <span class="cf">if</span> v.d <span class="op">&gt;</span> u.d <span class="op">+</span> w(u, v):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>    v.d <span class="op">=</span> u.d <span class="op">+</span> w(u, v)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>    v.pi <span class="op">=</span> u</span></code></pre></div>
<p>Note that this preserves the distance and predecessor invariants as this only changes if a better path is found.</p>
<h2 id="non-negative-weights">Non-negative weights</h2>
<p>If <span class="math inline">p_n = v_1 \to v_2 \to \cdots \to v_n</span> is a shortest path from <span class="math inline">v_1</span> to <span class="math inline">v_n</span> in a graph with no negative weights, then for all <span class="math inline">1 \le i \le n-1</span>, the prefix <span class="math display">
p_1 = v_1 \to \cdots \to v_i
</span> is in fact a shortest path from <span class="math inline">v_1</span> to <span class="math inline">v_i</span> and we must have <span class="math inline">\operatorname{weight}(p_i) \le \operatorname{weight}(p_n)</span>. This does not hold if some edges have negative weights! There might be edges of negative weight between <span class="math inline">p_i</span> and <span class="math inline">p_n</span>.</p>
<h2 id="dijkstras-algorithm">Dijkstra’s algorithm</h2>
<p>Solves the single-source shortest paths problem for non-negative weighted graphs. A more sophisticated variant called A* is commonly used in computer games.</p>
<ol type="1">
<li>Initialise <span class="math inline">s.d=0</span> and <span class="math inline">v.d=\infty</span> otherwise. Distance invariant is the same.</li>
<li>Initialise <span class="math inline">v.\pi=\textit{null}</span>. Predecessor invariant.</li>
<li>Visit each vertex in order of its distance from the source. For each outgoing edge, we relax along that edge.</li>
</ol>
<p>This is a generalisation of breadth first search to weighted graphs.</p>
<p>How can we efficiently find the next vertex to visit? Let <span class="math inline">S</span> be the set of visited vertices. We will maintain <span class="math inline">Q</span>, a priority queue of vertices <span class="math inline">V-S</span> keyed by <span class="math inline">v.d</span>.</p>
<p>This runs in <span class="math inline">O(E \lg V)</span> with a binary heap or <span class="math inline">O(E + V \lg V)</span> for a Fibonacci heap.</p>
<figure>
<img src="/assets/image-20200908090847384.png" alt="" /><figcaption>image-20200908090847384</figcaption>
</figure>
<h3 id="intuition">Intuition</h3>
<p>When we visit a vertex, we know that we have found the shortest path to <span class="math inline">u</span>. This means the predecessor <span class="math inline">v</span> of <span class="math inline">u</span> may have smaller distance, in which case we already visited it, or equal distance, in which case we have already found a shortest path to <span class="math inline">u</span> with the same length.</p>
<h2 id="negative-weights">Negative weights</h2>
<p>What if we had negative weights? Dijkstra’a algorithm might not relax edges in the correct order. That is, <span class="math inline">(u, v)</span> might be relaxed before we have a shortest path to <span class="math inline">u</span> so this relaxation would be incorrect.</p>
<p>Is the shortest path even well-defined for graphs with negative weight cycles? No, because we can just keep going around that negative cycle to reduce the weight.</p>
<p>Here is an example without negative weight cycles where Dijkstra’s algorithm fails. This goes in order <span class="math inline">a, c, d, b</span> which cannot find the correct shortest path.</p>
<figure>
<img src="/assets/image-20200908092238718.png" alt="" /><figcaption>image-20200908092238718</figcaption>
</figure>
<p>There are two problems here: we need to avoid negative weight cycles, and we need to relax in the correct order.</p>
<h2 id="bellman-ford">Bellman-Ford</h2>
<p>After initialisation, we have found all shortest paths which contain <span class="math inline">\le 0</span> edges. After relaxing each edge once, we have found all shortest paths containing <span class="math inline">\le 1</span> edges. Relaxing all the edges again finds shortest paths of length <span class="math inline">\le 2</span>. Continuing to <span class="math inline">V-1</span> times, we have all shortest paths containing <span class="math inline">\le V-1</span> edges.</p>
<p>The Bellman-Ford algorithm uses this to find single-source shortest paths on directed graphs which may contain negative weight edges. Additionally, it detects negative weight cycles and is able to return false if one is found.</p>
<figure>
<img src="/assets/image-20200908092800492.png" alt="" /><figcaption>image-20200908092800492</figcaption>
</figure>
<p>If there is a negative weight cycle, then even going through <span class="math inline">V-1</span> times is not enough to reduce the shortest path length to minimum. This is because <span class="math inline">V-1</span> is the maximum length of a shortest path with no cycles. We go through the edges once more and if we find another decrease (i.e. another shortest path), we return false.</p>
<p>This algorithm has time complexity <span class="math inline">O(V E)</span>, because it goes through the edges <span class="math inline">V-1</span> times. The worst-case of this is a fully connected graph where <span class="math inline">E=V^2</span>.</p>
<p>This is a generalisation of Dijkstra’s algorithm but runs in much slower time. Is there a more efficient way to find single-source shortest path if the graph is acyclic?</p>
<h2 id="priority-first-search">Priority first search</h2>
<p>This is a generalisation including <strong>Dijkstra’s algorithm</strong> and <strong>Prim’s algorithm</strong> for MST.</p>
<p>Vertices are visited in order of <em>priority</em> (for some definition of priority function). For example, the priority could be the weight (as in Prim’s) or the cumulative distance (as in Dijkstra’s).</p>
<figure>
<img src="/assets/image-20200908093707791.png" alt="" /><figcaption>image-20200908093707791</figcaption>
</figure><h1>Lecture 6</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 6 — Dynamic Programming</h1>
<p class="author">Kenton Lam</p>
<p class="date">2020-11-07</p>
</header>
-->


<h1 id="lecture-6-dynamic-programming">Lecture 6 — Dynamic Programming</h1>
<p>Dynamic programming is a method for efficiently solving problems with a certain structure.</p>
<ul>
<li>Problems have <strong>optimal substructure property</strong> where an optimal solution can be expressed in terms of optimal solutions to subproblems.</li>
<li>A naive recursive solution may be inefficient (exponential) due to repeated computation of subproblems.</li>
<li>Dynamic programming avoids recomputation of subproblems by storing them.</li>
</ul>
<p>Generally, this requires a deep understanding of the problem.</p>
<p>It can be constructed “<strong>bottom-up</strong>” which computes solutions to base-case subproblems first, then calculates larger subproblems until the goal can be computed. Massive speed improvements are possible, from exponential to polynomial time.</p>
<p>Another strategy is <strong>memoisation</strong> which is a top-down approach. It follows the same call stack as recursion and is often more ‘elegant’, but with worse constant factors than bottom-up.</p>
<h2 id="fibonacci-numbers">Fibonacci numbers</h2>
<p>The most basic dynamic programming is <span class="math display">
F(i) =\begin{cases}
1 , &amp;i = 1 \text{ or } i = 2,\\
F(i-1)+F(i-2), &amp;\text{else}.
\end{cases}
</span> In terms of code,</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">def</span> fib(n):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>    <span class="cf">if</span> n <span class="op">==</span> <span class="dv">1</span> <span class="kw">or</span> n <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>        <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>    <span class="cf">return</span> fib(n<span class="op">-</span><span class="dv">1</span>) <span class="op">+</span> fib(n<span class="op">-</span><span class="dv">2</span>)</span></code></pre></div>
<p>The corresponding recurrence is <span class="math display">
T(n) = T(n-1) + T(n-2) + \Theta(1) \in \Theta(1.6^n).
</span> <img src="/assets/image-20201107121537378.png" alt="image-20201107121537378" /></p>
<p>Looking at the recursive tree, we see there is significant overlap of the base cases being recalculated. For example, <span class="math inline">F(3)</span> is calculated three times in the calculation of <span class="math inline">F(6)</span>. The key idea is instead of recomputing these, we can store the calculations in an array and look it up later.</p>
<p>We can implement dynamic programming using arrays:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="kw">def</span> fib_dyn(n):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>    T <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> n</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>    T[<span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>    T[<span class="dv">2</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>, n<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>        T[i] <span class="op">=</span> T[i<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> T[i<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>    <span class="cf">return</span> T[n]</span></code></pre></div>
<p>The array stores the intermediate results and this obviously runs in time <span class="math inline">\Theta(n)</span>.</p>
<p>Alternatively, we can implement this using memoisation with a global variable <span class="math inline">T</span>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="kw">def</span> fib_init(N):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>    T <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> N</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>    T[<span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>    T[<span class="dv">2</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a>    </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a><span class="kw">def</span> fib_memo(n):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a>    <span class="cf">if</span> T[n] <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a>        T[n] <span class="op">=</span> fib_memo(n<span class="op">-</span><span class="dv">1</span>) <span class="op">+</span> fib_memo(n<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a>    <span class="cf">return</span> T[n]</span></code></pre></div>
<h2 id="general-principle">General principle</h2>
<p>If</p>
<ul>
<li>the problem has the optimal substructure property, and</li>
<li>its recursive solution has overlapping subproblems, then</li>
</ul>
<p>a dynamic programming solution may apply. The benefit of this is we get a much more efficient solution at the loss of some elegance.</p>
<h2 id="longest-common-subsequence">Longest common subsequence</h2>
<p>Suppose we have two strings and we want to find the longest (not necessarily contiguous) subsequence of characters shared between the two strings. For example, <span class="math display">
\begin{aligned}
S_1 &amp;= \text{A B C B C} \\ 
S_2 &amp;= \text{C A B B D}
\end{aligned}
</span> and here, <span class="math inline">\operatorname*{LCS}(S_1, S_2) = \text{ABB}</span>. This has applications in gene sequencing, file <code>diff</code>, and more.</p>
<p><strong>Assume</strong> we have already solved a smaller subproblem. First, we will develop a recursive description of LCS.</p>
<p>Suppose we know the LCS of S1 = “ABCBC” and S2 = “CABBD”. Then, what is the LCS of <em>S1.E</em> and <em>S2.E</em>? If we add the same character to both, then the LCS will be the same LCS but with E appended to the end, i.e. <em>LCS(S1, S2).E</em>.</p>
<p>What if we added different letters? Note that LCS(S1.X, S2.Y) is <em>not necessarily</em> LCS(S1, S2). We need to be careful if S2 already contains an X, for example. More formally, to calculate LCS(S1.X, S2.Y) where X is not Y we need to recursively look at both possibilities: <span class="math display">
\begin{aligned}
\operatorname*{LCS}(S_1.X, S_2.Y)=\max\{\operatorname*{LCS}(S_1.X, S_2),\ \operatorname*{LCS}(S_1, S_2.Y)\}
\end{aligned}
</span> under the assumption we already have the solutions to smaller subproblems.</p>
<p>The base case is if at least one of the argument strings is empty, the LCS is empty.</p>
<p>Below, we put it all together and return the length of the string instead of the string itself.</p>
<figure>
<img src="/assets/image-20201107145605147.png" alt="" /><figcaption>image-20201107145605147</figcaption>
</figure>
<p>In pseudocode, this can be elegantly written as</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="kw">def</span> lcs_length_rec(s1, s2, i, j):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> j <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a>    <span class="cf">if</span> s1[i] <span class="op">==</span> s2[j]:</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">+</span> lcs_length_rec(s1, s2, i<span class="op">-</span><span class="dv">1</span>, j<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a>    <span class="cf">return</span> <span class="bu">max</span>(lcs_length_rec(s1, s2, i<span class="op">-</span><span class="dv">1</span>, j), lcs_length_rec(s1, s2, i, j<span class="op">-</span><span class="dv">1</span>))</span></code></pre></div>
<p>Unfortunately, this is <span class="math inline">\Omega(2^{\min (n, m})</span>. We have solved an optimisation problem by finding optimal solutions to subproblems, so we can see it has the optimal substructure property.</p>
<p>We can rewrite this using bottom-up dynamic programming and a <span class="math inline">n \times m</span> integer array, because the recursive function has two arguments with <span class="math inline">n</span> and <span class="math inline">m</span> distinct values. Filling in this matrix from the smaller indices (i.e. smaller <span class="math inline">i</span> and <span class="math inline">j</span>) to the larger indices will solve the problem, which runs in <span class="math inline">\Theta(nm)</span> time.</p>
<h2 id="matrix-chain-multiplication">Matrix chain multiplication</h2>
<p>Recall that to compute <span class="math inline">AB</span> where <span class="math inline">A</span> is <span class="math inline">p \times q</span> and <span class="math inline">B</span> is <span class="math inline">q \times r</span>, then this takes time <span class="math inline">pqr</span>. Although <span class="math display">
M_1(M_2\cdot M_3) = (M_1\cdot M_2)M_3,
</span> the grouping may have a significant impact on the number of operations needed.</p>
<p>Suppose we have a sequence of matrices <span class="math inline">M_1, M_2, M_3, M_4</span> and need to evaluate their product. Matrix multiplication is not commutative but it is associative. What association minimises the total cost of multiplying these matrices? Assume that dimensions of adjacent matrices are compatible.</p>
<p>Before we get started, how long would a naive solution take? How many ways are there of multiplying <span class="math inline">n</span> matrices together? Let this be <span class="math inline">N</span>. Writing out a few terms, <span class="math display">
\begin{aligned}
N(1) &amp;= N(2) =1\\
N(3) &amp;= N(1) \times N(1)=2\\
N(4) &amp;= N(1) \times N(3) + N(2) \times N(2) + N(3) \times N(1)=5
\end{aligned}
</span> In general, <span class="math display">
N(n) = \sum_{i=1}^{n-1}N(i) \times N(n-i).
</span> This is tricky, but to get a lower bound, we have <span class="math inline">N(1) = 1</span> so <span class="math display">
\begin{aligned}
N(n) \ge N(1) \times N(n-1) + N(n-1)\times N(1) = 2N(n-1)\in \Omega(2^n).
\end{aligned}
</span> This is not great, can we find a dynamic programming solution? First we need to express it as a recursive problem, then identify overlapping subproblems.</p>
<h3 id="dynamic-programming">Dynamic programming</h3>
<p><strong>Subproblems</strong>: Let <span class="math inline">C_{ij}</span> be the minimum cost of multiplying the matrices between <span class="math inline">i</span> and <span class="math inline">j</span>, i.e. <span class="math inline">M_1 M_{i+1}\ldots M_j</span>. The solution is given by <span class="math inline">C_1,n</span>.</p>
<p><strong>Base cases:</strong> <span class="math inline">C_{ii}=0</span> for all <span class="math inline">i</span> because no operations are needed with a single matrix.</p>
<p><strong>Recursive case:</strong> To compute <span class="math inline">C_{ij}</span>, we can split the matrices <span class="math inline">i</span> to <span class="math inline">j</span> into different groupings, with lengths <span class="math inline">a</span> and <span class="math inline">b</span> with <span class="math inline">a, b \ge 1</span> and <span class="math inline">a+b = j-i+1</span>. We take the minimum of the recursive cost of each of these groupings added with the upfront cost of multiplying the two resultant matrices.</p>
<p>Specifically, <span class="math display">
C_{ij} = \min \{\ C_{i,i+k} + C_{i+k+1,j} + p_{i-1}p_{k+1}p_{j} : k \in 1, \ldots, j-i+1\ \}.
</span> This can be made slightly nicer by using <span class="math inline">i \le k &lt; j</span>. This is incredibly inefficient to compute directly (even worse than our lower bound above), but is the intuition we need.</p>
<p>A key insight here is that <span class="math inline">C_{ij}</span> depends on <span class="math inline">C_{ij}</span> and <span class="math inline">C_{k+1,j}</span>. We can draw a matrix of <span class="math inline">C</span> and fill it <em>bottom-up</em> starting with the diagonals, corresponding to <span class="math inline">C_{ii}=0</span>.</p><h1>Lecture 7</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 7 — Dynamic Programming 2</h1>
<p class="author">Kenton Lam</p>
<p class="date">2020-11-07</p>
</header>
-->


<h1 id="lecture-7-dynamic-programming-2">Lecture 7 — Dynamic Programming 2</h1>
<h2 id="all-pairs-shortest-paths">All-pairs shortest paths</h2>
<p>We want to find the shortest path between all pairs of nodes in a graph. Doing this naivlely with Dijkstra’s algorithm from each node is <span class="math inline">V</span> times of <span class="math inline">O(E + V \log V)</span> which is <span class="math inline">O(V^3)</span>. If we wanted to handle negative weights as well, we need to use Bellman-Ford which is <span class="math inline">V</span> times of <span class="math inline">O(V^3)</span>, so <span class="math inline">O(V^4)</span>.</p>
<p>We can do better by solving APSP including negative weights in <span class="math inline">O(V^3)</span> using something called Floyd-Warshall.</p>
<h2 id="recursive-formulation">Recursive formulation</h2>
<p>Suppose there are <span class="math inline">N</span> vertices numbered <span class="math inline">1, \ldots, N</span> and the graph is represented as an adjacency matrix where the weight from a vertex to itself is 0 and no edge is infinity.</p>
<p>Recursively defining this problem, a path from <span class="math inline">i</span> to <span class="math inline">j</span> is either:</p>
<ul>
<li>an empty path if <span class="math inline">i = j</span> of <span class="math inline">\langle i \rangle</span>, or</li>
<li>a path with more than one edge which is made up of a path <span class="math inline">p</span> from <span class="math inline">i</span> to some <span class="math inline">k</span> and an edge <span class="math inline">(k, j)</span>.</li>
</ul>
<p>The weight of this path in total is <span class="math inline">\operatorname*{weight}(p) + \operatorname*{weight}(k, j)</span>. The key insight is that if the shortest path <span class="math inline">\langle i, \ldots, k, j \rangle</span> has <span class="math inline">m</span> edges, then the path from <span class="math inline">i</span> to <span class="math inline">k</span> has at most <span class="math inline">m-1</span> edges and it is a <em>shortest</em> path from <span class="math inline">i</span> to <span class="math inline">k</span>.</p>
<p>Defining this recursively, let <span class="math inline">\operatorname*{shortestPath}(i, j)^m</span> or <span class="math inline">s(i, j)^m</span> be the weight of the shortest path from <span class="math inline">i</span> to <span class="math inline">j</span> using at most <span class="math inline">m</span> edges. As a result,</p>
<ul>
<li><span class="math inline">s(i, j)^0</span> is 0 if <span class="math inline">i=j</span> or <span class="math inline">\infty</span> otherwise, and</li>
<li><span class="math inline">s(i, j)^m = \min_{k \in V} (s(i, k)^{m-1}+w(k, j))</span>.</li>
</ul>
<p>Therefore, <span class="math inline">s(i, j)^1</span> is just the weight from <span class="math inline">i</span> to <span class="math inline">j</span>. We can use a <em>three-dimensional</em> array for the three arguments. This matrix will have dimensions <span class="math inline">n \times n \times n</span>. The pseudocode below would run in time <span class="math inline">\Theta(n^4)</span>.</p>
<figure>
<img src="/assets/image-20201107200932716.png" alt="" /><figcaption>image-20201107200932716</figcaption>
</figure>
<p>In fact, we don’t need to calculate for all <span class="math inline">m</span>. As a slight improvement, we can double <span class="math inline">m</span> at each step while still solving the problem. This gives us an asymptotic tight bound of <span class="math inline">\Theta(n^3 \log n)</span>.</p>
<h2 id="floyd-warshall">Floyd-Warshall</h2>
<p>Why did we define our subproblems in terms of path length? What if we defined it in terms of which intermediate nodes were in the path?</p>
<p>Let <span class="math inline">s(i,j,k)</span> be the weight of the shortest path from <span class="math inline">i</span> to <span class="math inline">j</span> using intermediate vertices <span class="math inline">1, \ldots, k</span>. Then, to extend <span class="math inline">k</span> to <span class="math inline">k+1</span>, we just need to consider if the path from <span class="math inline">i</span> to <span class="math inline">j</span> via <span class="math inline">k+1</span> is shorter than the previous shortest path. Specifically, <span class="math display">
\begin{aligned}
s(i, j, 0) &amp;= w(i, j) \\ 
s(i, j, k+1) &amp;= \min \{s(i, j, k),\ s(i, k+1, k) + s(k+1, j,k)\}.
\end{aligned}
</span> This removes the need to minimise over <span class="math inline">V</span>, significantly speeding up the algorithm. The code below runs in <span class="math inline">\Theta(n^3)</span> where <span class="math inline">n</span> is the number of vertices.</p>
<figure>
<img src="/assets/image-20201107201907760.png" alt="" /><figcaption>image-20201107201907760</figcaption>
</figure>
<p>The base case is <span class="math inline">D^{(0)}</span> which is just the weights of the graph.</p>
<figure>
<img src="/assets/image-20201107202104051.png" alt="" /><figcaption>image-20201107202104051</figcaption>
</figure>
<p>Note that when filling in the matrix <span class="math inline">D^{(1)}</span>, the 1 column and row stays constant because these are paths ending or starting at <span class="math inline">1</span> and the shortest path will not contain <span class="math inline">1</span> as an intermediate vertex.</p>
<p>To fill in the <span class="math inline">i,j</span>-th cell of <span class="math inline">D^{(1)}</span>, use the minimum of <span class="math inline">D^{(0)}_{ij}</span> and <span class="math inline">D^{(0)}_{i1}+D^{(0)}_{1j}</span>, i.e. the path through node <span class="math inline">1</span>.</p>
<figure>
<img src="/assets/image-20201107202918825.png" alt="" /><figcaption>image-20201107202918825</figcaption>
</figure>
<h2 id="johnsons-algorithm">Johnson’s algorithm</h2>
<p>This is <span class="math inline">\Theta(V^3)</span> worst case but for sparse graphs, it can be <span class="math inline">O(V^2 \log V)</span> using an adjacency list representation. The strategy is</p>
<ul>
<li>reweight to eliminate negative weight edges,</li>
<li>add a new “source” vertex, then</li>
<li>run Dijkstra’s algorithm from each node.</li>
</ul>
<p>This has relatively high overheads (constant factors).</p>
<h2 id="dp-vs-greedy-algorithms">DP vs greedy algorithms</h2>
<p>A dynamic programming solution usually applies to optimisation problems with optimal substructure property and overlapping subproblems.</p>
<p>Some optimisation problems with optimal substructure have the <strong>greedy-choice property</strong>. That is, given a larger problem, we know which subproblem will lead to the optimal solution without explicitly computing all the subproblems. To solve the problem, we can make a greedy (locally optimal) choice.</p>
<p>This is more efficient by making a greedy choice and solving only the chosen subproblem. If applicable, this can significantly reduce the runtime from dynamic programming because there are much fewer subproblems to solve.</p>
<h3 id="examples-of-greedy-algorithms">Examples of greedy algorithms</h3>
<p>For example, Prim’s algorithm works by selecting the minimum weight edge leaving the tree. Selecting the minimum weight edge is a greedy choice.</p>
<p>Similarly, Kruskal’s minimum spanning tree algorithm is a greedy algorithm by selecting the minimum weight edge which does not create a cycle.</p>
<p>Dijkstra’s algorithm is also a greedy algorithm by selecting the vertex with minimum distance from the priority queue.</p>
<h2 id="activity-selection">Activity selection</h2>
<p>Suppose we have a list of activities of the form (start, finish) and we want to choose a combination of these which are not overlapping and maximise the number of activities.</p>
<p>Process:</p>
<ol type="1">
<li>Sort by finishing time in <span class="math inline">\Theta(n \log n)</span>.</li>
<li>Add a compatible activity to the set, starting with the activity finishing first.</li>
</ol>
<p>This is possible because we surely want to choose the activity which finishes earliest to maximise possibilities at later points in time, regardless of the start time. We do not care to maximise time spent doing activities, only the number of activities done.</p>
<p>The loop is <span class="math inline">\Theta(n)</span> so the initial sort dominates the runtime with <span class="math inline">\Theta(n \log n)</span>.</p>
<h2 id="knapsack-problem">Knapsack problem</h2>
<p>Suppose we have a set of items each with a value <span class="math inline">v</span> and weight <span class="math inline">w</span> and we need to fill a knapsack while maximising the value such that the total weight is no more than <span class="math inline">W</span>.</p>
<p>Here are two variants:</p>
<ul>
<li>fractional knapsack where we can take fractions of items, and</li>
<li>binary knapsack where we can only take all or none of a particular item.</li>
</ul>
<p>In <em>fractional knapsack</em>, the greedy choice is just to fill the knapsack with the item which has the highest value to weight ratio. This does not work for binary knapsack.</p>
<figure>
<img src="/assets/image-20201107210131460.png" alt="" /><figcaption>image-20201107210131460</figcaption>
</figure><h1>Lecture 8</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 8 — Amortised Analysis</h1>
<p class="author">Kenton Lam</p>
<p class="date">2020-11-07</p>
</header>
-->


<h1 id="lecture-8-amortised-analysis">Lecture 8 — Amortised Analysis</h1>
<h2 id="amortised-analysis">Amortised analysis</h2>
<p>We’ve analysed algorithms “one-off”. However, we often use data structures and call their methods many times. Using different data structures can impact the running time of the algorithm.</p>
<p>Amortised analysis is essentially looking at the average complexity or time used for a <em>series</em> of operations. It has origins from financial contexts, for example a mortgage’s cost is split across a much longer time period.</p>
<p>Using naive analysis of a sequence of operations might assume the worst case all the time. In practice, the worst case may be very rare. Amortised analysis considers sequences of operations, typically those modifying a data structure.</p>
<p>Some examples include Java’s ArrayList, ArrayDeque, and C++’s vector which have operations running in <em>amortised constant time</em>.</p>
<h3 id="dynamic-table">Dynamic table</h3>
<p>The most simple example is a dynamic table, which is capable of storing an unknown number of items by doubling its capacity when it is full. Typically, inserting is constant-time but sometimes a reallocation and copy is needed.</p>
<p>The best-case is <span class="math inline">\Theta(1)</span> and worst-case is <span class="math inline">\Theta(n)</span> if resizing is necessary. To insert <span class="math inline">n</span> elements, the worst case would be <span class="math inline">\Theta(n^2)</span>? However, it is clear the <em>most</em> of the operations would be <span class="math inline">\Theta(1)</span>.</p>
<p>Formally, we are analysing</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>    insert(e[i])</span></code></pre></div>
<p>for some sequence of items <span class="math inline">e</span>. The vast majority of the operations are constant-time, but how many are not? This will depend on <span class="math inline">i</span>.</p>
<p>As a more precise analysis, we can let <span class="math inline">c_i</span> be the cost of the <span class="math inline">i</span>-th insertion which will be 1 most of the time but <span class="math inline">i</span> if <span class="math inline">i-1</span> was an exact power of 2.</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">i</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>size</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>4</td>
<td>8</td>
</tr>
<tr class="even">
<td><span class="math inline">c_i</span></td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>1</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>This is the <strong>aggregate method</strong> for amortised analysis. The cost of <span class="math inline">n</span> insertions is <span class="math display">
\begin{aligned}
\sum_{i=1}^n c_i \le n + \sum_{j=1}^{\lceil \lg n \rceil}2^{j-1} \le 3n = \Theta(n)
\end{aligned}
</span> The average cost of each insert operation is <span class="math inline">\Theta(n) / n = \Theta(1)</span>. What if we start mixing insert and delete operations? In that case, we can no longer use this summation because the cost will be different depending on the order of inserts and deletes.</p>
<h2 id="stack">Stack</h2>
<p>Consider a stack with two operations: push and pop. Suppose we add a multipop operation:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="kw">def</span> multipop(S, k):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>    <span class="cf">while</span> S <span class="kw">and</span> k <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>        pop(S)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>        k <span class="op">=</span> k <span class="op">-</span> <span class="dv">1</span></span></code></pre></div>
<p>The multipop operation can be <span class="math inline">O(n)</span> if <span class="math inline">n=k</span>. Clearly, any sequence of <span class="math inline">n</span> stack operations must be <span class="math inline">O(n^2)</span>. However, we can get a better bound.</p>
<p>We can use some details here:</p>
<ul>
<li>multipop only iterates while the stack is not empty, and</li>
<li>each element is pushed and popped exactly once.</li>
</ul>
<p>We are interested in analysing a sequence of <span class="math inline">n</span> mixed operations:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>    push(...) <span class="kw">or</span> pop(...) <span class="kw">or</span> multipop(...)</span></code></pre></div>
<p>Two methods we could use here are:</p>
<ul>
<li>the accounting method, which focuses on the operations, or</li>
<li>the potential method, which focuses on the data structure.</li>
</ul>
<h3 id="accounting-method">Accounting method</h3>
<p>Consider the actual cost <span class="math inline">c_i</span> of each operation. This is 1 for push/pop and <span class="math inline">\min(\operatorname*{size}(S), k)</span> for multipop. We can assign an <em>amortised cost</em> <span class="math inline">\hat c_i</span> for each method as 2 for push and 0 for the others. Here, we add the cost of the eventual pop upfront in the push method, using the rule that an item is pushed then popped exactly once. Then, the cost of the pop is already “paid” in the pop so “cost” nothing.</p>
<p>For any sequence of operations, the amortised cost must be an upper bound on the actual cost. We can use this in place of the complicated actual cost and say that a sequence of <span class="math inline">n</span> operations has amortised time <span class="math inline">O(n)</span>.</p>
<p>To prove this, we must show that running total of the amortised costs is greater than or running total of the sum of the actual costs. That is, for all sequences and <span class="math inline">n</span>, <span class="math display">
\sum_{i=1}^n \hat c_i \ge \sum_{i=1}^n c_i
</span> Intuitively, the extra “credit” in the push pays for any later (multi)pop.</p>
<h3 id="potential-method">Potential method</h3>
<p>This is a third method we can use. It finds a <strong>potential function</strong> on the data structure. The amortised cost is the actual cost plus change in potential. We write this as <span class="math display">
\hat c_i = c_i + \Phi(D_i) - \Phi(D_{i-1}).
</span> As before, the amortised cost must be an upper bound on the actual cost. Thus, <span class="math display">
\sum_{i=1}^n \hat c_i = \sum_{i=1}^n (c_i + \Phi(D_i)-\Phi(D_{i-1})) = \sum_{i=1}^n c_i + \Phi(D_n) - \Phi(D_0).
</span> We need to show that <span class="math inline">\Phi(D_i) \ge \Phi(D_0)</span> for all <span class="math inline">i</span>. This can be trivially satisfied if <span class="math inline">\Phi(D_0)=0</span> and <span class="math inline">\Phi(D_i) \ge 0</span>.</p>
<p>Returning to our stack, the actual costs are <span class="math inline">1</span>, <span class="math inline">1</span>, and <span class="math inline">k&#39;</span>. Define <span class="math inline">\Phi(S)</span> as the number of elements in the stack and the change in potential is <span class="math inline">1</span> for push, <span class="math inline">-1</span> for pop, and <span class="math inline">-k&#39;</span> for multipop.</p>
<p>The amortised cost is the actual cost added to the change in potential, giving us push is 2, pop is 0, and multipop is 0. Of course, <span class="math inline">\Phi(D_0)=0</span> because the stack starts out empty so the requirement is trivially satisfied. Thus, all stack operations have constant amortised time.</p>
<p>Here, this resulted in the same amortised costs as the accounting method.</p>
<h2 id="binary-counter">Binary counter</h2>
<p>Suppose we have a counter which counts by 1 in binary, from 0 to 1 to 10 and so on. The total cost is the number of bits flipped.</p>
<p>This works by flipping all the rightmost 1 bits to 0, then set a 1 in the next place.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="kw">def</span> increment(A, k):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a>    <span class="cf">while</span> i <span class="op">&lt;</span> k <span class="kw">and</span> A[i] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a>        A[i] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>        i <span class="op">=</span> i <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> k:</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a>        A[i] <span class="op">=</span> <span class="dv">1</span></span></code></pre></div>
<p>The worst case is if we have a chain of ones and this will take time <span class="math inline">n</span>, the length of <span class="math inline">A</span>. Doing <span class="math inline">n</span> increments would be <span class="math inline">n^2</span> then? Of course not!</p>
<p>Let’s see how we can think of this.</p>
<h3 id="aggregate-method">Aggregate method</h3>
<p>We want to count the number of flips. After <span class="math inline">n</span> increments, the number of times bit <span class="math inline">i</span> is flipped is <span class="math inline">\lfloor n/2^i \rfloor</span>. Bit 0 flips every time, 1 flips every second time, 2 flips every 4th increment, and so on.</p>
<figure>
<img src="/assets/image-20201108000211444.png" alt="" /><figcaption>image-20201108000211444</figcaption>
</figure>
<p>Hence, the total cost is <span class="math display">
\sum_{i=0}^{k-1}\left\lfloor\frac n {2^i}\right\rfloor \le 
\sum_{i=0}^{k-1}\frac n {2^i} &lt; n \sum_{i=0}^\infty \frac 1 {2^i} = 2n \in O(n).
</span> The aggregate method is writing the actual cost using a summation.</p>
<h3 id="accounting-method-1">Accounting method</h3>
<p>The actual cost is the number of low-order 1s plus one for the final flip of 0 to 1.</p>
<p>To determine the amortised cost, let each flip from 0 to 1 have amortised cost of 2 and let each flip from 1 to 0 have cost 0. The amortised cost of increment is then 2.</p>
<p>We must show this is an upper bound on the actual cost. Flips from 1 to 0 must occur after that flip has already been flipped and hence paid for.</p>
<h2 id="array-resizing">Array resizing</h2>
<p>We return to this using the accounting method.</p>
<h3 id="accounting-method-2">Accounting method</h3>
<p>The actual cost is as before. We can charge a cost of 3 for the <span class="math inline">i</span>-th insertion which is made up of 1 for the insert, and 2 for later doubling (1 for copying itself and 1 for copying an older element). We need to account for old elements because their credit would already have been used in the previous doubling, so we only have spare money from the most recent half.</p>
<p>This is essentially manually applying the cost upfront at each operation.</p>
<p>The invariant is that the “bank balance” never drops below zero. This is the same as the sum of amortised costs being an upper bound for sum of actual costs.</p>
<h2 id="potential-method-1">Potential method</h2>
<p>We need to determine a potential function. Here, this can be <span class="math inline">\Phi(D_i)=2i - 2^{\lceil \log i \rceil}</span>. Assume that <span class="math inline">2^{\lceil \log 0 \rceil}=0</span> which trivially satisfies <span class="math inline">\Phi(D_0)=0</span> and <span class="math inline">\Phi(D_i) \ge 0</span>. This is essentially the same as a “bank balance” from before.</p>
<p>The amortised cost of the <span class="math inline">i</span>-th insertion is <span class="math display">
\begin{aligned}
\hat c_i &amp;= c_i - \Phi(D_i)-\Phi(D_{i-1})  \\ 
&amp;= \begin{cases}
i, &amp; i-1\text{ is a power of 2}, \\ 
1, &amp;\text{else},
\end{cases}
+(2i-2^{\lceil \log i\rceil })
-(2(i-1)-2^{\lceil \log (i-1)\rceil })
\end{aligned}
</span> Looking at each case, if <span class="math inline">i-1</span> is an exact power of <span class="math inline">2</span>, <span class="math display">
\begin{aligned}
\hat c_i &amp;= i+2 - 2^{\lceil \log i \rceil} + 2^{\lceil \log (i-1)\rceil} \\ 
&amp;= i+2 - 2(i-1) + (i-1) \\ 
&amp;= 3
\end{aligned}
</span> If <span class="math inline">i-1</span> is not an exact power of 2, the ceiling makes the power the same and so <span class="math display">
\hat c_i=3.
</span> Therefore, <span class="math inline">n</span> inserts take <span class="math inline">\Theta(n)</span> amortised time in the worst case.</p><h1>Lecture 9</h1>

  <!--
<header id="title-block-header">
<h1 class="title">Lecture 9 — Computational Complexity</h1>
<p class="author">Kenton Lam</p>
<p class="date">2020-11-08</p>
</header>
-->


<h1 id="lecture-9-computational-complexity">Lecture 9 — Computational Complexity</h1>
<h2 id="polynomial-time">Polynomial time</h2>
<p>We call polynomial time algorithms <strong>tractable</strong>. In practice, large polynomial algorithms are not tractable. However, if an algorithm has a polynomial-time solution, there is usually an <em>efficient</em> polynomial-time solution.</p>
<p>For example, matrix multiplication has been improved from <span class="math inline">\Theta(N^3)</span> to something less.</p>
<p>The class of problems solvable in polynomial time on a serial random-access machine (which we have been using) is the same as</p>
<ul>
<li>the polynomial-time problems on an abstract Turing machine, and</li>
<li>the polynomial-time problems on a parallel computer when number of processors grows polynomially with input size.</li>
</ul>
<p>So the computational model we use to execute our algorithm is largely irrelevant for the analysis.</p>
<p>Polynomials are <strong>closed</strong> under addition, multiplication, and composition. Thus, if the output of a polynomial-time algorithm is used as input for another polynomial-time algorithm, the resultant algorithm is also polynomial-time.</p>
<h2 id="examples">Examples</h2>
<p>Which problems have polynomial-time algorithms? Problems which look similar (in nature) may not be.</p>
<p>We can find shortest paths from a single source in a directed graph in <span class="math inline">O(VE)</span> time, however the <em>longest</em> simple path between two vertices is difficult. Even determining if a simple path exists with at least a given number of vertices is difficult (NP-complete).</p>
<p>An Euler tour in a connected directed graph is a cycle in which each edge of the graph is traversed exactly once. Finding an Euler tour can be done in <span class="math inline">O(E)</span> time. A Hamiltonian cycle in a graph is a simple cycle which contains all vertices in a graph. Determining <em>whether</em> a graph has a Hamiltonian cycle is NP-complete.</p>
<h3 id="cnf-vs-3-cnf-satisfiability">2-CNF vs 3-CNF satisfiability</h3>
<p>A boolean formula like <span class="math display">
(x_1 \vee \neg x_2) \wedge (\neg x_1 \vee x_3) \wedge (\neg x_2 \vee \neg x_3)
</span> may have a satisfying assignment of <span class="math inline">x_1, x_2, x_3</span> such that it evaluates to 1. The formula above does, e.g. 1 0 1.</p>
<p>A boolean formula is in conjunctive normal form (CNF) if it is a conjunction (and) of clauses, each of which is a disjunction )or) of literals, where a literal is a variable or a negation. Any boolean formula can be converted to CNF.</p>
<p>A <span class="math inline">k</span>-CNF formula is one where each disjunction (or) has exactly <span class="math inline">k</span> literals. The example above is 2-CNF. Determining whether a 3-CNF is satisfiable is NP-complete.</p>
<h2 id="background">Background</h2>
<p>Which problems have polynomial-time algorithms?</p>
<p>In complexity theory, we study <em>classes</em> of problems (not algorithms themselves). We study <strong>decision problems</strong> (those with a yes or no) rather than optimisation problems. We will focus on concrete problems rather than abstract problems.</p>
<p>Optimisation problems usually have closely related decision problems. If the optimisation problem is “easy”, the decision problem is also “easy”. If the optimisation problem is “hard”, the decision problem is also “hard”.</p>
<h3 id="example">Example</h3>
<p>A shortest path problem is an optimisation problem. We can can convert “find the shortest path from <span class="math inline">u</span> to <span class="math inline">v</span>” to “is there a path from <span class="math inline">u</span> to <span class="math inline">v</span> with at most <span class="math inline">k</span> edges”?</p>
<p>If we can solve the optimisation problem, we can just take the length of the path and return 1 or 0 depending on whether the path’s length exceeds <span class="math inline">k</span>. Shortest path optimisation is solvable in polynomial-time and so the shortest path decision problem is solvable in polynomial-time.</p>
<h3 id="definitions">Definitions</h3>
<p><strong>Definition.</strong> An <strong>abstract problem</strong> is a binary relation on a set <span class="math inline">I</span> of problem instances and a set <span class="math inline">S</span> of problem solutions. Note there may be multiple solutions for a given instance.</p>
<p><strong>Definition.</strong> An <strong>abstract decision problem</strong> is a function from an instance set <span class="math inline">I</span> to the solution set <span class="math inline">\{0,1\}</span>. Any given instance has a unique 0 or 1 solution.</p>
<h3 id="encodings">Encodings</h3>
<p>We need to represent problem instances in a some way a computer can understand.</p>
<p><strong>Definition.</strong> An <strong>encoding</strong> of a set <span class="math inline">S</span> of abstract objects is a mapping from <span class="math inline">S</span> to the set of binary strings <span class="math inline">\{0,1\}^*</span>. For example, we can encode a natural number in its binary representation. More complicated objects can be represented as a binary string of its components.</p>
<p><strong>Definition.</strong> A <strong>concrete problem</strong> is a problem whose instance set is the set of binary strings <span class="math inline">\{0,1\}^*</span>.</p>
<p><strong>Definition.</strong> An algorithm <strong>solves</strong> a concrete problem in time <span class="math inline">O(T(n))</span> if, given an instance <span class="math inline">i</span> of length <span class="math inline">n=|i|</span>, the algorithm can produce a solution in <span class="math inline">O(T(n))</span> time.</p>
<p><strong>Definition.</strong> A concrete problem is <strong>polynomial-time solvable</strong> if there exists an algorithm to solve it in <span class="math inline">O(n^k)</span> time for some constant <span class="math inline">k</span>. Here, <span class="math inline">n</span> is the size of the input.</p>
<p><strong>Definition.</strong> The <strong>complexity class <em>P</em></strong> is the set of <em>concrete decision problems</em> which are <em>polynomial-time solvable</em>.</p>
<p>An abstract decision problem <span class="math inline">Q</span> maps an instance <span class="math inline">i</span> to 0 or 1: <span class="math inline">Q \in I \to \{0,1\}</span>. An encoding makes abstract problems concrete. Given <span class="math inline">e : I \to \{0,1\}^*</span>, we can map an abstract problem to a concrete problem with <span class="math display">
e(Q) \in \{0,1\}* \to \{0,1\}.
</span> And we can solve an abstract instance <span class="math inline">i</span> with <span class="math display">
e(Q)(e(i)) = Q(i).
</span> Some binary strings may not map to an instance. We just assume such invalid strings map to <span class="math inline">0</span>.</p>
<p>Ideally, the complexity would be independent of encoding but this is not the case.</p>
<p>As an example, suppose we take an integer <span class="math inline">k</span> as the sole input to an algorithm and it has running time <span class="math inline">\Theta(k)</span>. If we encode it as binary number, the size of the input is <span class="math inline">n=\lfloor \log k \rfloor + 1</span> and the complexity of the resulting algorithm is <span class="math inline">\Theta(k) = \Theta(2^n)</span>. If we instead use a unary representation, the size of the input is <span class="math inline">n=k</span> and the complexity is <span class="math inline">\Theta(n)</span>.</p>
<p>In practice, we want to rule out expensive encodings like unary.</p>
<p>We assume a <strong>standard encoding</strong> where the encoding of an integer is polynomially related to its binary encoding, the encoding of a finite set is polynomially related to its encoding as a list of its elements enclosed in braces and separated by commas (as binary bytes). The notation <span class="math inline">\langle G \rangle</span> is the standard encoding of <span class="math inline">G</span>.</p>
<p><strong>Definition.</strong> A <strong>polynomial-time computable function</strong> is a function <span class="math inline">f : \{0,1\}^* \to \{0,1\}^*</span> if there exists a polynomial-time algorithm <span class="math inline">A</span> that given any <span class="math inline">x \in \{0,1\}^*</span> produces <span class="math inline">f(x)</span>.</p>
<h3 id="reductions">Reductions</h3>
<p><strong>Definition.</strong> A <strong>polynomial-time reduction <em>F</em></strong> is a polynomial-time algorithm which reduces an instance <span class="math inline">\alpha</span> of a problem <span class="math inline">A</span> to an instance <span class="math inline">\beta</span> of the problem <span class="math inline">B</span>. Then, if <span class="math inline">\beta=F(\alpha)</span> then <span class="math inline">A(\alpha)=B(\beta)</span> for all <span class="math inline">\alpha</span>. Therefore, if <span class="math inline">B</span> is easy, then <span class="math inline">A</span> is easy as well.</p>
<p>We can use reductions to show how hard a problem is. Suppose we know that no polynomial-time algorithm algorithm exists to solve <span class="math inline">A</span> and we can reduce <span class="math inline">A</span> to <span class="math inline">B</span> in polynomial time. As a result of this, <span class="math inline">B</span> cannot have a polynomial time algorithm. If <span class="math inline">A</span> is hard, <span class="math inline">B</span> is hard as well.</p>
<p>To determine if a problem <span class="math inline">A</span> is in <em>P</em>, we can come up with a polynomial-time algorithm for <span class="math inline">A</span> or reduce <span class="math inline">A</span> to <span class="math inline">B</span> in polynomial-time where <span class="math inline">B</span> is a polynomial-time solvable problem.</p>
<h2 id="nondeterministic-polynomial-np">Nondeterministic polynomial (NP)</h2>
<p><strong>Definition</strong>. The <strong>complexity class <em>NP</em></strong> is the set of concrete decision problems for which a solution can be verified in polynomial time. The naming comes because if you could somehow nondeterministically choose the correct solution, we would be able to solve it in polynomial-time.</p>
<p>Problems which we can’t even verify a solution in polynomial time are unlikely to have a polynomial-time (efficient) algorithm to solve it.</p>
<p><em>P</em> is trivially a subset of <em>NP</em>, because if we can solve a problem we can just compute and compare solutions. The question is this: are there some problems where a solution is quick to verify but <em>can’t</em> be implemented efficiently? Most people believe there are (so <span class="math inline">P \ne NP</span>), but we cannot prove it yet.</p>
<p>This is the famous <strong>P = NP</strong> problem. There is a $1,000,000 prize if you can prove or disprove this statement.</p>
<p>Many problems are in NP but we do not know whether they are in P.</p>
<h3 id="travelling-salesman-problem">Travelling salesman problem</h3>
<p>The TSP is to find the shortest (weighted) path which visits each vertex exactly once and returns where it starts. We will deal with the related decision problem which is “is there a tour of weight at most <span class="math inline">k</span>?”. A solution is a list of vertices in the order visited.</p>
<p>This decision problem can be verified by checking <span class="math inline">S</span> contains every vertex, adjacent vertices are connected, and the sum of weights is at most <span class="math inline">k</span>. This can all be done in polynomial time.</p>
<p>Therefore, <span class="math inline">\text{TSP} \in \textit{NP}</span>, but we do not know if this is in <span class="math inline">P</span>. This is despite TSP being of practical commercial importance and decades of research.</p>
<h2 id="np-hard">NP-Hard</h2>
<p><strong>Definition.</strong> A concrete decision problem <span class="math inline">B</span> is <strong>NP-hard</strong> if <em>every</em> problem <span class="math inline">\textit{A} \in \textit{NP}</span> is polynomial-time reducible to <span class="math inline">B</span>.</p>
<p>Thus, if we can solve just one one NP-hard problem, then we can solve <em>all</em> NP problems in polynomial time and P = NP.</p>
<h2 id="np-complete">NP-Complete</h2>
<p><strong>Definition.</strong> A concrete decision problem <span class="math inline">B</span> is <strong>NP-complete</strong> if and only if it is <em>NP-hard</em> and it is in <em>NP</em>. These are the <em>hardest</em> problems in NP.</p>
<p>Are there even any NP-complete problems? In fact, TSP is in NPC so every other problem in NP can be reduced to TSP in polynomial time.</p>
<p>The fact that NPC is non-empty is towards proving P = NP, but it is unlikely that this is the case. If we can show that a problem is NPC, it is reasonable to stop looking for an efficient algorithm (otherwise we would’ve shown P = NP).</p>
<h3 id="circuit-sat">Circuit-SAT</h3>
<p>The first NPC problem proven via Cook’s theorem: “the circuit-SAT problem is NP-complete”.</p>
<p>A circuit is a logical circuit of AND/OR gates, etc. Cook showed that <em>any</em> problem in NP can be reduced to circuit-SAT in polynomial-time. Thus, if we can solve circuit-SAT in polynomial-time, we can solve anything in NP in polynomial-time.</p>
<p>The problem is does there exist an assignment of inputs which produces a 1 as output.</p>
<figure>
<img src="/assets/image-20201108131250998.png" alt="" /><figcaption>image-20201108131250998</figcaption>
</figure>
<p>The proof is difficult and it involves encoding problem instances as binary strings, but this is a very useful result.</p>
<p>Proving that other problems are NPC is then easy. Let <span class="math inline">X</span> be an arbitrary problem. If circuit-SAT is polynomial-time reducible to <span class="math inline">X</span>, then <span class="math inline">X</span> is NP-hard.</p>
<p>This is because if we can solve <span class="math inline">X</span> in polynomial-time, then we can solve circuit-SAT and hence any NP problem in polynomial-time.</p>
<p>To show a problem is in NPC, we will also need to show <span class="math inline">X</span> is in NP.</p>
<h3 id="reductions-1">Reductions</h3>
<p>The graph below shows transitive polynomial-time reductions. Alternatively, read the arrow as “can be implemented using”. Therefore, circuit-SAT is no harder than SAT and so on, so since circuit-SAT is NP-hard, SAT is NP-hard.</p>
<figure>
<img src="/assets/image-20201108131719689.png" alt="" /><figcaption>image-20201108131719689</figcaption>
</figure>
<p>Reducing circuit-SAT to SAT is done by writing the output of each gate as an intermediate variable as below. This can be done in polynomial time by doing constant work per gate.</p>
<p>Reducing SAT to 3-CNF-SAT is possible because every boolean formula can be polynomially reduced to 3-CNF.</p>
<p>We can reduce 3-CNF-SAT to CLIQUE can be done by drawing a graph where the vertices are the literals (variables or their negation). An edge exists between vertices if those variables are <em>compatible</em>. If there exists a clique of size <span class="math inline">k</span>, then there exists an assignment of variables for 3-CNF with <span class="math inline">k</span> clauses.</p>
<p>CLIQUE can be reduced to VERTEX-COVER since they are dual problems. If we take the complement graph (same vertices, opposite edges), then a cover is just the clique of the graph complement. A cover is a minimum set of vertices which touch every edge.</p>
<p>VERTEX-COVER can be reduced to HAM-CYCLE but the graph is hideous.</p>
<p>We can reduce HAM-CYCLE to TSP. Now, we don’t want just any path which visits all vertices but we want the least cost one. We can just add weights to each edge and solve TSP where <span class="math inline">k</span> is the one minus the number of edges. Similarly, we can assign an edge which exists a weight 0 and a non-existent edge weight 1, then look for a tour with weight 0.</p>
<h2 id="overview">Overview</h2>
<p>The classes of problems:</p>
<ul>
<li><em>P</em> is polynomial time solvable.</li>
<li><em>NP</em> is polynomial time verifiable.</li>
<li><em>NP-hard</em> if all problems in NP can be polynomial-time reducible to the NP-hard problem.</li>
<li><em>NPC</em> is NP-hard and NP.</li>
</ul>
<p><span class="math display">
\textit{P} \subseteq \textit{NP}, \quad \textit{NPC} \subseteq \textit{NP},\quad \textit{P}\overset?=\textit{NP}.
</span></p>
<p>If we can reduce an NP-hard problem to a problem X, then X must also be NP-hard.</p>
<h2 id="approximations">Approximations</h2>
<h3 id="approximate-tsp">Approximate TSP</h3>
<p>If our TSP graph satisfies the <em>triangle quality</em>, for all <span class="math inline">u,v,w</span>, the cost of going directly between vertices is no more than an indirect path, <span class="math display">
c(u,w) \le c(u,v) + c(v,w),
</span> then we can approximate it using the minimum spanning tree. This will return a tour of no more than twice the cost of the optimal tour.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">def</span> approx_tsp_tour(G,c):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>    r <span class="op">=</span> <span class="bu">next</span>(G.V)         <span class="co"># choose a random root vertex</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>    T <span class="op">=</span> mst_prim(G, c, r) <span class="co"># run prim&#39;s algorithm to find MST</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>    L <span class="op">=</span> preorder(T)       <span class="co"># preorder walk of T</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>    <span class="cf">return</span> L              <span class="co"># hamiltonian cycle which visits in order L</span></span></code></pre></div>
<p>Firstly, if we weren’t worried about repeating vertices, we could just walk along the MST tree. This would be the minimum cost of joining all the vertices. To make it a tour, we visit the vertices in the order of a preorder traversal. Due to the triangle inequality, this gives us the bound on the approximation cost.</p>
<figure>
<img src="/assets/image-20201108160523208.png" alt="" /><figcaption>image-20201108160523208</figcaption>
</figure>
    <p><small>Generated at 11/8/2020, 8:22:04 AM.</small></p>
  </body>
</html>
